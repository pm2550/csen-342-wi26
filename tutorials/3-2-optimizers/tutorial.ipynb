{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3-2: The Great Optimizer Race\n",
    "\n",
    "**Course:** CSEN 342: Deep Learning  \n",
    "**Topic:** Optimization Algorithms (SGD, Momentum, Adam, AdamW) and Schedulers\n",
    "\n",
    "## Objective\n",
    "In the lecture, we discussed how standard **Gradient Descent** can get stuck in local minima or oscillate in ravines. We introduced advanced optimizers like **Momentum**, **RMSProp**, and **Adam** designed to overcome these issues.\n",
    "\n",
    "In this tutorial, we will:\n",
    "1.  **Visualize the Landscape:** Define a tricky 2D loss function (the **Beale Function**) that mimics the difficult terrain of high-dimensional loss surfaces. It has large flat regions where gradients are tiny and sharp peaks.\n",
    "2.  **Run the Race:** Compare the trajectories of SGD, Momentum, Adam, and the modern standard **AdamW**. We will measure not just *if* they converge, but *how fast* (in steps) they reach the minimum.\n",
    "3.  **Tune the Schedule:** Compare **Step Decay** vs. **Cosine Annealing**. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The Loss Landscape\n",
    "\n",
    "We use the **Beale Function**, a classic benchmark for optimization algorithms. \n",
    "$$ f(x, y) = (1.5 - x + xy)^2 + (2.25 - x + xy^2)^2 + (2.625 - x + xy^3)^2 $$\n",
    "\n",
    "It has a global minimum at $(3, 0.5)$. The challenge is that the gradient in the flat regions is very small, making it hard for standard SGD to make progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "# Define the Beale Function using PyTorch tensors\n",
    "def beale(x, y):\n",
    "    return (1.5 - x + x*y)**2 + (2.25 - x + x*y**2)**2 + (2.625 - x + x*y**3)**2\n",
    "\n",
    "# Visualization Utility\n",
    "def plot_surface(paths=None, names=None):\n",
    "    # Generate grid for plotting\n",
    "    x = np.linspace(-4.5, 4.5, 200)\n",
    "    y = np.linspace(-4.5, 4.5, 200)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = beale(X, Y)\n",
    "\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Log scale contour plot to see the minima better\n",
    "    plt.contourf(X, Y, Z, levels=50, norm=LogNorm(), cmap='viridis', alpha=0.7)\n",
    "    plt.colorbar(label='Log Loss')\n",
    "    \n",
    "    # Plot the global minimum (known at x=3, y=0.5)\n",
    "    plt.plot(3, 0.5, 'r*', markersize=20, label='Global Min (3, 0.5)', zorder=10)\n",
    "\n",
    "    # Plot optimizer paths if provided\n",
    "    if paths:\n",
    "        # Distinct colors for tracks\n",
    "        colors = ['white', 'cyan', 'magenta', 'orange']\n",
    "        linestyles = ['-', '--', '-.', ':']\n",
    "        \n",
    "        for i, (path, name) in enumerate(zip(paths, names)):\n",
    "            path = np.array(path)\n",
    "            # zorder ensures tracks are drawn in order (later ones on top)\n",
    "            plt.plot(path[:, 0], path[:, 1], color=colors[i % len(colors)], \n",
    "                     linestyle=linestyles[i % len(linestyles)],\n",
    "                     linewidth=2, markersize=0, label=name, zorder=i+2, alpha=0.9)\n",
    "            \n",
    "            # Plot start and end points\n",
    "            plt.plot(path[0, 0], path[0, 1], 'ko', markersize=5) # Start\n",
    "            plt.plot(path[-1, 0], path[-1, 1], 'x', color=colors[i], markersize=10, markeredgewidth=2) # End\n",
    "    \n",
    "    plt.xlabel('x (Parameter 1)')\n",
    "    plt.ylabel('y (Parameter 2)')\n",
    "    plt.title('Optimization Trajectories on Beale Function')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.grid(True, linestyle='--', alpha=0.3)\n",
    "    plt.xlim(-4.5, 4.5)\n",
    "    plt.ylim(-4.5, 4.5)\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the empty landscape\n",
    "plot_surface()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: The Race (Comparing Optimizers)\n",
    "\n",
    "We will run different PyTorch optimizers starting from the same difficult point $(-3, -3)$ and see how they navigate towards the star (minimum).\n",
    "\n",
    "### Why add AdamW?\n",
    "Although your lecture slides cover Adam, modern deep learning research (specifically for Transformers like BERT and GPT) has largely shifted to **AdamW**.\n",
    "\n",
    "**The difference:** Standard Adam implements L2 regularization by adding it to the loss function. AdamW decouples the weight decay from the gradient update step. This subtle mathematical difference results in significantly better generalization performance for deep models. As a student of Deep Learning, it is critical to look beyond the textbook and stay updated with the latest research papers (like *Loshchilov & Hutter, ICLR 2019* which introduced AdamW).\n",
    "\n",
    "We will compare:\n",
    "1.  **SGD:** Standard Gradient Descent.\n",
    "2.  **SGD + Momentum:** Adding velocity to power through flat regions.\n",
    "3.  **Adam:** The standard adaptive optimizer.\n",
    "4.  **AdamW:** The modern standard with decoupled weight decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optimizer(optimizer_class, lr=0.01, steps=2000, gradient_clip=False, **kwargs):\n",
    "    # 1. Initialize Parameters at a bad starting point (-3, -3)\n",
    "    params = torch.tensor([-3.0, -3.0], requires_grad=True)\n",
    "    \n",
    "    # 2. Initialize the Optimizer\n",
    "    optimizer = optimizer_class([params], lr=lr, **kwargs)\n",
    "    \n",
    "    path = []\n",
    "    dist_to_target = []\n",
    "    \n",
    "    # Target location (Global Min)\n",
    "    target = torch.tensor([3.0, 0.5])\n",
    "    \n",
    "    for i in range(steps):\n",
    "        # Save current position\n",
    "        current_pos = params.detach().numpy().copy()\n",
    "        path.append(current_pos)\n",
    "        \n",
    "        # Calculate distance to target for analysis\n",
    "        dist = torch.norm(params - target).item()\n",
    "        dist_to_target.append(dist)\n",
    "        \n",
    "        # Optimization Step\n",
    "        optimizer.zero_grad()\n",
    "        loss = beale(params[0], params[1])\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip gradients (if requested)\n",
    "        if gradient_clip:\n",
    "            torch.nn.utils.clip_grad_norm_([params], gradient_clip)\n",
    "            \n",
    "        optimizer.step()\n",
    "        \n",
    "        # Early stopping if we are extremely close\n",
    "        if dist < 1e-3:\n",
    "            break\n",
    "            \n",
    "    return path, dist_to_target\n",
    "\n",
    "print(\"Optimization Harness Ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Running the Race\n",
    "* **SGD** usually needs a very high learning rate to move anywhere on this surface, or it gets stuck. We will give it `lr=0.01`.\n",
    "* **Adam/AdamW** are adaptive, so they can handle `lr=0.1` effectively here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the racers\n",
    "print(\"Running optimizers... this may take a moment.\")\n",
    "\n",
    "# SGD often needs momentum to move in flat regions\n",
    "path_sgd, dist_sgd = run_optimizer(torch.optim.SGD, lr=0.01, steps=25000)\n",
    "path_mom, dist_mom = run_optimizer(torch.optim.SGD, lr=0.001, steps=25000, momentum=0.9)\n",
    "path_adam, dist_adam = run_optimizer(torch.optim.Adam, lr=0.5, steps=25000)\n",
    "path_adamw, dist_adamw = run_optimizer(torch.optim.AdamW, lr=0.1, steps=25000)\n",
    "\n",
    "# Visualize Paths\n",
    "plot_surface(\n",
    "    paths=[path_sgd, path_mom, path_adam, path_adamw],\n",
    "    names=['SGD', 'SGD+Momentum', 'Adam', 'AdamW']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Quantitative Analysis\n",
    "Visuals are great, but numbers tell the truth. Let's see how many steps it took to reach the target (or how close they got)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(name, path, dists):\n",
    "    final_dist = dists[-1]\n",
    "    steps = len(path)\n",
    "    status = \"Converged\" if final_dist < 1e-2 else \"Did not converge\"\n",
    "    print(f\"{name:<15} | Steps: {steps:<5} | Final Dist: {final_dist:.4f} | Status: {status}\")\n",
    "\n",
    "print(\"Race Results (Target: Distance < 0.01):\")\n",
    "print(\"-\"*65)\n",
    "print_stats(\"SGD\", path_sgd, dist_sgd)\n",
    "print_stats(\"SGD+Momentum\", path_mom, dist_mom)\n",
    "print_stats(\"Adam\", path_adam, dist_adam)\n",
    "print_stats(\"AdamW\", path_adamw, dist_adamw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "You likely observed that **SGD** and **SGD+Momentum** failed to converge. The fact that your final distance is `nan` (Not a Number) means the optimization didn't just get \"stuck\"—it exploded. The parameters shot off to infinity.\n",
    "\n",
    "The Beale function is deceptive. While it has a large flat plateau, it also has extremely steep \"walls\" or cliffs at the edges of the domain.\n",
    "- **What happened:** Your SGD optimizer likely wandered into one of these steep regions. The gradient there was massive. Because SGD blindly multiplies the gradient by the learning rate ($w \\leftarrow w - \\eta \\nabla L$), it took a gigantic step, landing in a spot with an even larger gradient (or infinity), causing the values to become `nan`.\n",
    "- **Why Adam survived:** Adam divides the update by the square root of the running variance of the gradients ($v_t$). When gradients get huge, $v_t$ gets huge, effectively scaling down the step size and preventing the explosion.\n",
    "\n",
    "*How do we fix SGD?*\n",
    "\n",
    "We have two options:\n",
    "\n",
    "1. **Lower the learning rate.** This is a simple fix but doesn't always work. SGD is extremely sensitive to the learning rate. 0.01 is too aggressive for the steep parts of this specific landscape. We can try lowering it by an order of magnitude. Note that, with a lower LR, SGD will move very slowly on the flat plateau, so it might still fail to reach the target in 25,000 steps.\n",
    "2. **Gradient clipping.** This is a standard technique in training RNNs and deep networks (falls in the category of `optimization tricks`). It caps the maximum size of the gradient update, preventing explosions while allowing you to keep a higher learning rate.\n",
    "\n",
    "Let's give it a try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the racers\n",
    "print(\"Running optimizers... this may take a moment.\")\n",
    "\n",
    "# SGD often needs momentum to move in flat regions\n",
    "path_sgd, dist_sgd = run_optimizer(torch.optim.SGD, lr=0.01, steps=25000, gradient_clip=True)\n",
    "path_mom, dist_mom = run_optimizer(torch.optim.SGD, lr=0.001, steps=25000, momentum=0.9, gradient_clip=True)\n",
    "path_adam, dist_adam = run_optimizer(torch.optim.Adam, lr=0.5, steps=25000)\n",
    "path_adamw, dist_adamw = run_optimizer(torch.optim.AdamW, lr=0.1, steps=25000)\n",
    "\n",
    "# Visualize Paths\n",
    "plot_surface(\n",
    "    paths=[path_sgd, path_mom, path_adam, path_adamw],\n",
    "    names=['SGD', 'SGD+Momentum', 'Adam', 'AdamW']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(name, path, dists):\n",
    "    final_dist = dists[-1]\n",
    "    steps = len(path)\n",
    "    status = \"Converged\" if final_dist < 1e-2 else \"Did not converge\"\n",
    "    print(f\"{name:<15} | Steps: {steps:<5} | Final Dist: {final_dist:.4f} | Status: {status}\")\n",
    "\n",
    "print(\"Race Results (Target: Distance < 0.01):\")\n",
    "print(\"-\"*65)\n",
    "print_stats(\"SGD\", path_sgd, dist_sgd)\n",
    "print_stats(\"SGD+Momentum\", path_mom, dist_mom)\n",
    "print_stats(\"Adam\", path_adam, dist_adam)\n",
    "print_stats(\"AdamW\", path_adamw, dist_adamw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "**SGD** sometimes gets stuck in the flat plateau because the gradient is too small to push it forward. However, a high learning rate can easily make the gradients explode. Gradient clipping can come to its rescue.\n",
    "\n",
    "**Momentum**  helps carry the optimizer through the flat region using velocity. However, if the learning rate is too high, even when clipping gradients, momentum can spiral and overshoot the target. It accelerates down the steep cliffs, and by the time it reaches the minimum, it's moving so fast it flies right past it. It then tries to turn around, builds up speed again, and oscillates or diverges. Try running it with `lr=0.1` and see what happens.\n",
    "\n",
    "**Adam** normalizes the update by the variance (second moment). It is like a \"cautious driver.\" On the steep cliffs of the Beale function, the variance ($v_t$) becomes huge. Adam sees this high variance and drastically scales down the step size to prevent explosion. It is being \"safe.\"\n",
    "\n",
    "**AdamW** applies Weight Decay (L2 Regularization). Why was it the slowest? Weight decay pushes parameters toward zero (0, 0). The global minimum is at (3, 0.5). AdamW is trying to move to 3, but weight decay is constantly pulling it back toward 0. It's fighting a headwind!\n",
    "\n",
    "For many Deep Learning problems, **Adam/AdamW** typically converge the fastest because they adapt the learning rate for each parameter individually. In some cases (like the Beal problem), you might find that SGD sometimes beats Adam in raw step count. This is because Adam is \"safe\"—it dampens the step size when gradients are steep to prevent explosion. However, notice that to make **SGD+Momentum** converge, we had to carefully tune (lower) the learning rate to 0.001. For Adam, we could aggressively raise it to 0.5 without diverging. **Adam's primary advantage is robustness**: it works reasonably well across a huge range of learning rates, whereas SGD requires precise tuning to avoid stalling or exploding. That is why it is preferred in most modern DL training.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Learning Rate Schedulers\n",
    "\n",
    "In the lecture, we saw **Step Decay** (Slide 24). However, modern research often favors **Cosine Annealing**. The example below shows the effective learning rate across 50 epochs given the two chosen learning rate schedulers.\n",
    "\n",
    "**Why Cosine Annealing?**\n",
    "Step decay causes sudden \"shocks\" to the model training (the abrupt drops in loss seen in Slide 24). Cosine annealing lowers the learning rate smoothly, following a cosine curve. This ensures the model spends time exploring at all scales of learning rates, often finding better, flatter minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Import utility functions\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "from utils import download_fashion_mnist\n",
    "\n",
    "# Device config\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 1. Load Data (Using ../data as requested)\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "download_fashion_mnist()\n",
    "trainset = torchvision.datasets.FashionMNIST(root='../data', train=True, download=False, transform=transform)\n",
    "# Use a subset for speed\n",
    "train_subset, _ = torch.utils.data.random_split(trainset, [5000, 55000])\n",
    "trainloader = torch.utils.data.DataLoader(train_subset, batch_size=64, shuffle=True)\n",
    "\n",
    "# 2. Define a Simple Model\n",
    "model = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(28*28, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 10)\n",
    ").to(device)\n",
    "\n",
    "# Helper to record LR changes\n",
    "def simulate_schedule(scheduler_cls, steps=50, **kwargs):\n",
    "    # Re-init optimizer for each run\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "    scheduler = scheduler_cls(optimizer, **kwargs)\n",
    "    lrs = []\n",
    "    \n",
    "    for _ in range(steps):\n",
    "        # Mock Training Step\n",
    "        optimizer.step()   # 1. Update weights first\n",
    "        scheduler.step()   # 2. Update LR second (Fixes UserWarning)\n",
    "        \n",
    "        # Record LR\n",
    "        lrs.append(optimizer.param_groups[0]['lr'])\n",
    "        \n",
    "    return lrs\n",
    "\n",
    "print(\"Simulating Schedules...\")\n",
    "\n",
    "# 3. Run Simulations\n",
    "# StepLR: Multiply LR by 0.5 every 10 epochs\n",
    "lrs_step = simulate_schedule(optim.lr_scheduler.StepLR, step_size=10, gamma=0.5)\n",
    "\n",
    "# Cosine: Smoothly decay from 0.1 to 0 over 50 epochs\n",
    "lrs_cosine = simulate_schedule(optim.lr_scheduler.CosineAnnealingLR, T_max=50, eta_min=0)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(lrs_step, marker='o', label='Step Decay')\n",
    "plt.plot(lrs_cosine, marker='x', label='Cosine Annealing')\n",
    "plt.title(\"Comparison of Learning Rate Schedules\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "By visualizing the optimizers and schedulers, we can see why choices matter. While the lectures cover the foundational math, implementation details (like `AdamW` and step ordering) are critical for getting code to work in the real world."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "342wi26",
   "language": "python",
   "name": "342wi26"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

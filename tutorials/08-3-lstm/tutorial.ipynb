{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 8-3: The Memory Test â€“ \"LSTM vs. The Vanishing Gradient\"\n",
    "\n",
    "**Course:** CSEN 342: Deep Learning  \n",
    "**Topic:** Long-Term Memory, Vanishing Gradients, and Gating Mechanisms\n",
    "\n",
    "## The Problem: Vanishing Gradients\n",
    "In the lecture, we learned that Vanilla RNNs struggle with long sequences. When you backpropagate through time (BPTT) over many steps, the gradient signal is repeatedly multiplied by the weight matrix. \n",
    "* If the weights are small, the gradient **vanishes** to zero (the model stops learning).\n",
    "* If the weights are large, the gradient **explodes** to infinity (the model destabilizes).\n",
    "\n",
    "This makes it nearly impossible for a standard RNN to connect an input at $t=0$ to an output at $t=100$.\n",
    "\n",
    "## The Solution: LSTMs\n",
    "Long Short-Term Memory (LSTM) networks were explicitly designed to solve this. They introduce a **Cell State** ($C_t$) that acts as a superhighway for gradients, allowing information to flow unchanged over long distances.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The \"Adding Problem\" Task\n",
    "\n",
    "To prove this, we will use a classic benchmark called **The Adding Problem** (Hochreiter & Schmidhuber, 1997).\n",
    "\n",
    "**The Setup:**\n",
    "We feed the network a long sequence (length **100**). Each time step has 2 input numbers:\n",
    "1.  **Signal:** A random number between 0 and 1.\n",
    "2.  **Marker:** A binary flag (0 or 1). \n",
    "\n",
    "**The Rules:**\n",
    "* In the entire sequence of 100 steps, exactly **two** steps will have the Marker set to `1.0`.\n",
    "* All other steps have the Marker set to `0.0`.\n",
    "* The Goal: **Predict the sum of the two marked numbers.**\n",
    "\n",
    "**Why is this hard?**\n",
    "The first number might appear at $t=5$ and the second at $t=95$. The model must hold the first number in memory for 90 steps, ignoring all the noise in between, to perform the addition at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 1. DATA GENERATION ---\n",
    "class AddingProblemDataset(Dataset):\n",
    "    def __init__(self, seq_len=100, num_samples=10000):\n",
    "        self.num_samples = num_samples\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        # Features: [Batch, Seq, 2]\n",
    "        # Channel 0: Random values in [0, 1]\n",
    "        self.x = torch.rand((num_samples, seq_len, 2))\n",
    "        \n",
    "        # Channel 1: The markers. All 0s initially.\n",
    "        self.x[:, :, 1] = 0.\n",
    "        \n",
    "        # Target: [Batch, 1]\n",
    "        self.y = torch.zeros((num_samples, 1))\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            # Pick two distinct random indices to mark\n",
    "            indices = np.random.choice(seq_len, size=2, replace=False)\n",
    "            \n",
    "            # Mark them with 1.0\n",
    "            self.x[i, indices[0], 1] = 1.0\n",
    "            self.x[i, indices[1], 1] = 1.0\n",
    "            \n",
    "            # Calculate sum of the marked values\n",
    "            val1 = self.x[i, indices[0], 0]\n",
    "            val2 = self.x[i, indices[1], 0]\n",
    "            self.y[i] = val1 + val2\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "# Settings\n",
    "SEQ_LEN = 100\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "dataset = AddingProblemDataset(seq_len=SEQ_LEN, num_samples=10000)\n",
    "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(f\"Dataset Created. Sequence Length: {SEQ_LEN}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: The Baseline (Random Guessing)\n",
    "\n",
    "If the model cannot learn to remember, what will it do?\n",
    "\n",
    "It will try to minimize the MSE (Mean Squared Error) by predicting the **expected value** (average) of the sum.\n",
    "* Each number is uniform in $[0, 1]$, so the average is $0.5$.\n",
    "* The sum of two numbers averages to $1.0$.\n",
    "* The MSE of constantly guessing $1.0$ is approximately **0.167**.\n",
    "\n",
    "If our model loss gets stuck at **0.167**, it means it has failed to learn the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: The Model\n",
    "\n",
    "We use a generic `RecurrentModel` wrapper so we can easily swap between `nn.RNN` and `nn.LSTM`.\n",
    "\n",
    "### Critical Detail: Initialization\n",
    "By default, PyTorch initializes the LSTM's \"Forget Gate\" bias to 0. This means it starts with a 50% chance of forgetting at every step. For a sequence of 100 steps, this causes gradients to vanish just like an RNN.\n",
    "\n",
    "To give the LSTM a fighting chance, we perform **Forget Gate Bias Initialization**: we set it to `1.0`. This tells the untrained LSTM: *\"By default, keep memory intact.\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentModel(nn.Module):\n",
    "    def __init__(self, model_type, input_size=2, hidden_size=64):\n",
    "        super().__init__()\n",
    "        self.model_type = model_type\n",
    "        \n",
    "        if model_type == 'RNN':\n",
    "            self.rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size, batch_first=True)\n",
    "        elif model_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(input_size=input_size, hidden_size=hidden_size, batch_first=True)\n",
    "            self._init_weights()\n",
    "            \n",
    "        # Regression output (predicting a single number)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def _init_weights(self):\n",
    "        # Initialize Forget Gate bias to 1.0 to enable long-term memory\n",
    "        for name, param in self.rnn.named_parameters():\n",
    "            if \"bias\" in name:\n",
    "                n = param.shape[0]\n",
    "                start, end = n//4, n//2\n",
    "                param.data[start:end].fill_(1.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [Batch, Seq, 2]\n",
    "        out, _ = self.rnn(x)\n",
    "        \n",
    "        # We only care about the prediction at the FINAL time step\n",
    "        last_step = out[:, -1, :]\n",
    "        return self.fc(last_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: The Race\n",
    "\n",
    "We will now train both models side-by-side.\n",
    "\n",
    "* **Hyperparameters:** We use a slightly higher learning rate (0.005) to help the LSTM escape the \"average guessing\" plateau.\n",
    "* **Clipping:** We clip gradients for the RNN to prevent \"exploding gradients\" from causing numerical instability. This forces the RNN to rely on standard gradient flow, exposing the vanishing gradient issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def train_model(model_type, epochs=10):\n",
    "    model = RecurrentModel(model_type).to(device)\n",
    "    # Learning Rate 0.005 helps punch through the initial plateau\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    loss_history = []\n",
    "    \n",
    "    print(f\"\\n--- Training {model_type} ---\")\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            preds = model(x)\n",
    "            loss = criterion(preds, y)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Clip gradients to keep training stable\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "        avg_loss = epoch_loss / len(loader)\n",
    "        loss_history.append(avg_loss)\n",
    "        print(f\"Epoch {epoch+1}: Loss = {avg_loss:.4f}\")\n",
    "        \n",
    "    return loss_history\n",
    "\n",
    "# Run the Race\n",
    "rnn_losses = train_model('RNN', epochs=10)\n",
    "lstm_losses = train_model('LSTM', epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Visualization\n",
    "\n",
    "Let's compare the learning curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot RNN\n",
    "plt.plot(rnn_losses, label='Vanilla RNN', color='red', linestyle='--', linewidth=2)\n",
    "\n",
    "# Plot LSTM\n",
    "plt.plot(lstm_losses, label='LSTM', color='green', linewidth=3)\n",
    "\n",
    "# Add baseline line (Random Guessing MSE approx 0.167)\n",
    "plt.axhline(y=0.167, color='gray', linestyle=':', label='Baseline (Guessing)')\n",
    "\n",
    "plt.title(f\"Adding Problem Loss (Sequence Length = {SEQ_LEN})\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "**1. The Vanilla RNN (Red Dashed Line):** \n",
    "Notice how the RNN loss flattens out around **0.167**. It has failed to find the markers. Because the sequence is so long (100 steps), the gradient signal vanishes before it reaches the start of the sequence. The RNN gives up and simply guesses the average ($1.0$), achieving the baseline loss.\n",
    "\n",
    "**2. The LSTM (Green Solid Line):** \n",
    "The LSTM breaks through the baseline floor!  \n",
    "\n",
    "This confirms that the LSTM successfully learned to:\n",
    "1.  Identify the first marker.\n",
    "2.  Store its value in the **Cell State**.\n",
    "3.  Maintain that memory for ~50-90 steps.\n",
    "4.  Add it to the second marker when it appears.\n",
    "\n",
    "This ability to solve the \"Vanishing Gradient\" problem is why LSTMs (and Transformers) replaced simple RNNs in modern deep learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

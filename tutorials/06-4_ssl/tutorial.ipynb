{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 6-4: The Self-Supervised Evolution â€“ \"From SimCLR to SimSiam\"\n",
    "\n",
    "**Course:** CSEN 342: Deep Learning  \n",
    "**Topic:** Self-Supervised Learning (SSL), Contrastive Learning, and Siamese Networks\n",
    "\n",
    "## Objective\n",
    "Deep learning typically requires massive labeled datasets. **Self-Supervised Learning (SSL)** removes this bottleneck by generating its own labels from the data itself.\n",
    "\n",
    "The core idea is simple: **Invariance**. If we take an image of a dog and crop it, rotate it, or change its color, it is *still* an image of a dog. The network should output the same feature vector for both versions.\n",
    "\n",
    "In this tutorial, we will trace the evolution of modern SSL:\n",
    "1.  **SimCLR (2020):** Uses **Contrastive Learning**. It pulls augmentations of the *same* image together (Positives) and pushes augmentations of *different* images apart (Negatives).\n",
    "2.  **SimSiam (2021):** Removes the need for negative pairs entirely. It uses a **Siamese Network** with a **Stop-Gradient** operation to prevent the model from cheating (collapsing to a constant solution).\n",
    "\n",
    "We will evaluate our self-supervised model using a **Linear Probe**: training a simple classifier on the frozen features to see if the network learned meaningful concepts (like \"dog\" or \"plane\") without supervision.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The Engine (Data Augmentation)\n",
    "\n",
    "The most critical component of SSL is the augmentation pipeline. We need to generate two \"views\" ($x_i, x_j$) of every image. \n",
    "\n",
    "We use **CIFAR-10**. We will define a `SimCLRTransform` that applies:\n",
    "1.  Random Resized Crop (forces model to learn parts-to-whole).\n",
    "2.  Random Horizontal Flip.\n",
    "3.  Color Jitter (forces model to ignore color histograms).\n",
    "4.  Grayscale (optional, but helps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Config\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 128 # Larger batch sizes are better for SimCLR (more negatives)\n",
    "epochs = 5       # We keep it short for the tutorial; real SSL takes 100+ epochs\n",
    "proj_dim = 128   # Dimension of the projection head\n",
    "\n",
    "# 1. Define The Augmentation Wrapper\n",
    "class SimCLRTransform:\n",
    "    \"\"\"\n",
    "    Generates two different random augmentations of the same image.\n",
    "    \"\"\"\n",
    "    def __init__(self, size=32):\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(size=size, scale=(0.2, 1.0)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8),\n",
    "            transforms.RandomGrayscale(p=0.2),\n",
    "            transforms.ToTensor(),\n",
    "            # Standard CIFAR-10 Normalization\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "        ])\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # Return two views\n",
    "        return self.transform(x), self.transform(x)\n",
    "\n",
    "# 2. Load Data\n",
    "data_root = '../data'\n",
    "os.makedirs(data_root, exist_ok=True)\n",
    "\n",
    "# Train set (Unlabeled - we ignore the labels!)\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root=data_root, train=True, download=True, transform=SimCLRTransform()\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "# Test set (Labeled - for linear probe evaluation later)\n",
    "# Standard transform for testing\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root=data_root, train=False, download=True, transform=test_transform\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 3. Visualization\n",
    "def imshow(img):\n",
    "    # Un-normalize for display\n",
    "    img = img * torch.tensor([0.2023, 0.1994, 0.2010]).view(3, 1, 1) + torch.tensor([0.4914, 0.4822, 0.4465]).view(3, 1, 1)\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.axis('off')\n",
    "\n",
    "# Get a batch\n",
    "(x_i, x_j), _ = next(iter(train_loader))\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "for i in range(4):\n",
    "    plt.subplot(2, 4, i+1)\n",
    "    imshow(x_i[i])\n",
    "    plt.title(\"View 1\")\n",
    "    plt.subplot(2, 4, i+5)\n",
    "    imshow(x_j[i])\n",
    "    plt.title(\"View 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: SimCLR (Contrastive Learning)\n",
    "\n",
    "**Architecture:**\n",
    "1.  **Backbone ($f$):** A ResNet-18 (without the final classification layer). This extracts representation $h$.\n",
    "2.  **Projection Head ($g$):** A small MLP (Linear $\\to$ ReLU $\\to$ Linear) that maps $h$ to $z$.\n",
    "\n",
    "**Why the Projection Head?** \n",
    "Research shows that the contrastive loss destroys some information (like color/rotation) to achieve invariance. We want the *Backbone* ($h$) to keep that information for downstream tasks, so we perform the destructive contrastive step in the *Projection Head* ($z$) instead.\n",
    "\n",
    "**The Loss (NT-Xent):**\n",
    "For a pair of positive images $(i, j)$, we maximize similarity while minimizing similarity to all other $2(N-1)$ images in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimCLR(nn.Module):\n",
    "    def __init__(self, base_model=None):\n",
    "        super(SimCLR, self).__init__()\n",
    "        \n",
    "        # 1. Backbone (ResNet-18)\n",
    "        # We drop the final FC layer to get features directly\n",
    "        resnet = torchvision.models.resnet18(pretrained=False)\n",
    "        # CIFAR-10 is small (32x32), so we replace the first 7x7 conv with 3x3\n",
    "        # to avoid downsampling too aggressively at the start.\n",
    "        resnet.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        resnet.maxpool = nn.Identity() # Remove maxpool for small images\n",
    "        \n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        feature_dim = resnet.fc.in_features\n",
    "\n",
    "        # 2. Projection Head (MLP)\n",
    "        # Maps 512 -> 128\n",
    "        self.projection_head = nn.Sequential(\n",
    "            nn.Linear(feature_dim, feature_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(feature_dim, proj_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.projection_head(h)\n",
    "        return h, z\n",
    "\n",
    "# NT-Xent Loss Function\n",
    "def nt_xent_loss(z_i, z_j, temperature=0.5):\n",
    "    batch_size = z_i.shape[0]\n",
    "    \n",
    "    # Concatenate all features: [z_i, z_j] -> size (2N, D)\n",
    "    features = torch.cat([z_i, z_j], dim=0)\n",
    "    \n",
    "    # Calculate Cosine Similarity Matrix\n",
    "    features = F.normalize(features, dim=1)\n",
    "    similarity_matrix = torch.matmul(features, features.T) # (2N, 2N)\n",
    "    \n",
    "    # Remove self-similarity (diagonal)\n",
    "    # We construct a mask to ignore the diagonal\n",
    "    mask = torch.eye(2 * batch_size, dtype=torch.bool).to(device)\n",
    "    \n",
    "    # For each item i (0...N-1), its positive pair is at i + N\n",
    "    # For each item i+N, its positive pair is at i\n",
    "    labels = torch.cat([\n",
    "        torch.arange(batch_size) + batch_size,\n",
    "        torch.arange(batch_size)\n",
    "    ]).to(device)\n",
    "    \n",
    "    # Discard diagonal elements from similarity matrix for loss calculation\n",
    "    # This part can be tricky in PyTorch. A simpler way is to use CrossEntropy directly.\n",
    "    # The logits are the similarity scores / temperature\n",
    "    logits = similarity_matrix / temperature\n",
    "    \n",
    "    # We mask out the self-similarity by setting it to a very large negative number\n",
    "    logits.masked_fill_(mask, -9e15)\n",
    "    \n",
    "    # Cross Entropy calculates log_softmax automatically\n",
    "    loss = F.cross_entropy(logits, labels)\n",
    "    return loss\n",
    "\n",
    "model_simclr = SimCLR().to(device)\n",
    "optimizer_simclr = optim.Adam(model_simclr.parameters(), lr=1e-3)\n",
    "\n",
    "print(\"SimCLR Model Initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: The Evolution to SimSiam\n",
    "\n",
    "SimCLR requires **Negative Pairs** (the other images in the batch) to prevent collapse. If we only pulled positive pairs together, the network would just output `[0, 0, 0]` for every image, achieving 0 loss.\n",
    "\n",
    "**SimSiam** solves this *without* negatives. It uses two tricks:\n",
    "1.  **Predictor Head:** One branch has an extra MLP ($p$) that tries to predict the output of the other branch.\n",
    "2.  **Stop-Gradient:** We calculate loss against the *target* branch, but we **do not** backpropagate errors through the target branch. This turns the target branch into a stable \"teacher\" for the predictor.\n",
    "\n",
    "**Loss:** Negative Cosine Similarity.\n",
    "$$ D(p_1, z_2) = - \\frac{p_1}{\\|p_1\\|_2} \\cdot \\frac{z_2}{\\|z_2\\|_2} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimSiam(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimSiam, self).__init__()\n",
    "        \n",
    "        # Reuse the SimCLR backbone/projection structure\n",
    "        self.simclr_base = SimCLR()\n",
    "        feature_dim = self.simclr_base.projection_head[2].out_features\n",
    "\n",
    "        # 3. Predictor Head (MLP)\n",
    "        # Used ONLY in SimSiam on the \"student\" branch\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, feature_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # Get projections (z)\n",
    "        _, z1 = self.simclr_base(x1)\n",
    "        _, z2 = self.simclr_base(x2)\n",
    "        \n",
    "        # Predict the other view\n",
    "        p1 = self.predictor(z1)\n",
    "        p2 = self.predictor(z2)\n",
    "        \n",
    "        return p1, p2, z1.detach(), z2.detach() # DETACH IS CRITICAL\n",
    "    # The torch.Tensor.detach() method is used to separate a tensor from its current computation graph,\n",
    "    # effectively preventing any further gradient calculations from flowing back through that point. \n",
    "    # It returns a new tensor that shares the same underlying data storage as the original but has \n",
    "    # requires_grad=False. \n",
    "\n",
    "def negative_cosine_similarity(p, z):\n",
    "    p = F.normalize(p, dim=1)\n",
    "    z = F.normalize(z, dim=1)\n",
    "    return -(p * z).sum(dim=1).mean()\n",
    "\n",
    "model_simsiam = SimSiam().to(device)\n",
    "optimizer_simsiam = optim.Adam(model_simsiam.parameters(), lr=1e-3)\n",
    "\n",
    "print(\"SimSiam Model Initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Training Loop\n",
    "\n",
    "We will train SimSiam for a few epochs. (Training SimCLR follows the same loop structure, just different loss, but we will focus on SimSiam here as it is the more advanced method).\n",
    "\n",
    "**Note:** Real SSL training takes 100-1000 epochs. We will run 3 epochs just to verify the loss decreases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting SimSiam Training (Pre-training)...\")\n",
    "\n",
    "model_simsiam.train()\n",
    "losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for i, ((x1, x2), _) in enumerate(train_loader):\n",
    "        x1, x2 = x1.to(device), x2.to(device)\n",
    "        \n",
    "        optimizer_simsiam.zero_grad()\n",
    "        \n",
    "        # Forward Pass\n",
    "        # p1 predicts z2, p2 predicts z1\n",
    "        p1, p2, z1, z2 = model_simsiam(x1, x2)\n",
    "        \n",
    "        # Symmetric Loss\n",
    "        # L = 0.5 * D(p1, z2) + 0.5 * D(p2, z1)\n",
    "        loss = negative_cosine_similarity(p1, z2) / 2 + negative_cosine_similarity(p2, z1) / 2\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer_simsiam.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    losses.append(avg_loss)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] Loss: {avg_loss:.4f}\")\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.title(\"SimSiam Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Neg Cosine Similarity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: The Linear Probe Evaluation\n",
    "\n",
    "How do we know if the model learned anything? \n",
    "\n",
    "We freeze the backbone (disable gradient updates). We attach a fresh Linear Layer to the output of the backbone (features $h$). We train *only* this linear layer on the labeled training data for 1 epoch.\n",
    "\n",
    "If the accuracy is high (e.g., >30-40% for just 1 epoch of linear training), it means the backbone has learned to separate the classes in the feature space purely from unlabeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting Linear Probe Evaluation...\")\n",
    "\n",
    "# 1. Create a Linear Classifier on top of the FROZEN backbone\n",
    "class LinearProbe(nn.Module):\n",
    "    def __init__(self, backbone, feature_dim=512, num_classes=10):\n",
    "        super(LinearProbe, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.fc = nn.Linear(feature_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad(): # FREEZE BACKBONE\n",
    "            h = self.backbone(x).flatten(start_dim=1)\n",
    "        return self.fc(h)\n",
    "\n",
    "probe_model = LinearProbe(model_simsiam.simclr_base.backbone).to(device)\n",
    "probe_opt = optim.Adam(probe_model.fc.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 2. Reload data (Standard transform, not SimCLR transform, because we need labels now)\n",
    "train_ds_labeled = torchvision.datasets.CIFAR10(\n",
    "    root=data_root, train=True, download=True, \n",
    "    transform=test_transform # Use simple transform for probing\n",
    ")\n",
    "train_loader_labeled = DataLoader(train_ds_labeled, batch_size=128, shuffle=True)\n",
    "\n",
    "# 3. Train the Probe just for one epoch\n",
    "probe_model.train()\n",
    "for i, (imgs, labels) in enumerate(train_loader_labeled):\n",
    "    imgs, labels = imgs.to(device), labels.to(device)\n",
    "    \n",
    "    probe_opt.zero_grad()\n",
    "    preds = probe_model(imgs)\n",
    "    loss = criterion(preds, labels)\n",
    "    loss.backward()\n",
    "    probe_opt.step()\n",
    "    \n",
    "    if i % 100 == 0: print(f\"Probe Step {i}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# 4. Test Accuracy\n",
    "probe_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in test_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = probe_model(imgs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "acc = 100 * correct / total\n",
    "print(f\"\\nLinear Probe Accuracy: {acc:.2f}%\")\n",
    "print(\"(Random guessing is 10%. Anything above 30% proves SSL worked.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "**What just happened?**\n",
    "You trained a ResNet backbone without ever showing it a label (SimSiam phase). It learned to recognize patterns by figuring out that a cropped dog and a rotated dog are the \"same thing\".\n",
    "\n",
    "Then, you froze that backbone and trained a tiny linear layer. The fact that the linear layer could classify images with decent accuracy proves that the **backbone learned semantic features** (it clustered dogs near dogs and planes near planes in the vector space) purely through self-supervision."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

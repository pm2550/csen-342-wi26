{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 6-1: The Residual Revolution â€“ \"Building ResNet\"\n",
    "\n",
    "**Course:** CSEN 342: Deep Learning  \n",
    "**Topic:** Residual Networks, Skip Connections, and Bottleneck Layers\n",
    "\n",
    "## Objective\n",
    "As we saw in the previous lecture, simply stacking more layers doesn't always lead to better performance. In fact, very deep \"plain\" networks (20+ layers) often have *higher* training error than shallower ones due to optimization difficulties.\n",
    "\n",
    "The **ResNet** (Residual Network) architecture solved this by introducing the **Skip Connection**, allowing gradients to flow directly through the network.\n",
    "\n",
    "In this tutorial, we will:\n",
    "1.  **Implement the Residual Block:** Build the fundamental building block $H(x) = F(x) + x$.\n",
    "2.  **Assemble ResNet:** Stack these blocks to create a deep network (ResNet-18 style) for CIFAR-10.\n",
    "3.  **Implement the Bottleneck:** Build the advanced block used in ResNet-50/101/152 that uses 1x1 convolutions to reduce computation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Data\n",
    "\n",
    "We use the utility function to download CIFAR-10 safely on the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add parent directory to path to import utils\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "from utils import download_cifar10\n",
    "\n",
    "# 1. Pre-download Data\n",
    "download_cifar10()\n",
    "\n",
    "# 2. Prepare DataLoaders\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='../data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='../data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"Data loaded on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: The Basic Residual Block\n",
    "\n",
    "The core idea is to learn a **residual function** $F(x)$ rather than the direct mapping $H(x)$. We do this by adding the input $x$ to the output of the convolutional layers.\n",
    "\n",
    "**Structure:**\n",
    "1.  Conv3x3 (preserve spatial size)\n",
    "2.  BatchNorm + ReLU\n",
    "3.  Conv3x3\n",
    "4.  BatchNorm\n",
    "5.  **Add Input (Skip Connection)**\n",
    "6.  ReLU\n",
    "\n",
    "**Challenge:** If we change the number of channels (e.g., 64 -> 128) or spatial size (stride 2), we can't just add $x$ because the shapes don't match. We need a **Shortcut Connection** (usually a 1x1 conv) to resize $x$[cite: 4030]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1 # Used for ResNet sizing\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        # Main Path (F(x))\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        # Shortcut Path (x)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        \n",
    "        # If dimensions change (stride > 1 or channels increase), we need to adapt x\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # F(x)\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        \n",
    "        # + x (The Skip Connection)\n",
    "        out += self.shortcut(x)\n",
    "        \n",
    "        # Final ReLU\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: The Bottleneck Block\n",
    "\n",
    "For very deep networks (ResNet-50 and above), using two 3x3 convolutions becomes expensive. We use a **Bottleneck** design.\n",
    "\n",
    "**Structure (Slide 52):**\n",
    "1.  **1x1 Conv:** Reduce dimensions (squeeze).\n",
    "2.  **3x3 Conv:** Process features in lower dimensions.\n",
    "3.  **1x1 Conv:** Restore dimensions (expand).\n",
    "\n",
    "This allows the network to be much deeper with similar computational cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4 # Output channels are 4x input channels in the main path\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        # 1. Squeeze (1x1)\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        \n",
    "        # 2. Process (3x3)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        \n",
    "        # 3. Expand (1x1)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        \n",
    "        # Skip Connection\n",
    "        out += self.shortcut(x)\n",
    "        \n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Building ResNet\n",
    "\n",
    "Now we assemble the blocks into a full network. We follow the pattern of ResNet-18[cite: 4045]:\n",
    "1.  **Stem:** Initial Conv + BatchNorm + ReLU.\n",
    "2.  **Layers:** 4 groups of residual blocks, doubling the filters periodically.\n",
    "3.  **Head:** Global Average Pooling + Fully Connected Layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        # Stem\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # Layers (groups of blocks)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        \n",
    "        # Head\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Stem\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        \n",
    "        # Blocks\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        \n",
    "        # Head (Global Average Pooling)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "model = ResNet18().to(device)\n",
    "print(\"ResNet-18 Created.\")\n",
    "# Check parameter count\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total Parameters: {num_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Training\n",
    "\n",
    "We will train for a few epochs to verify convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "    print(f\"Epoch {epoch}: Loss: {train_loss/(batch_idx+1):.3f} | Acc: {100.*correct/total:.2f}%\")\n",
    "\n",
    "print(\"Starting Training...\")\n",
    "for epoch in range(1, 4): # Train for 3 epochs for demo\n",
    "    train(epoch)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "You have successfully implemented a modern ResNet from scratch. Key takeaways:\n",
    "1.  **Skip Connections** enable deep networks by creating a \"highway\" for gradients.\n",
    "2.  **Basic Blocks** ($3\\times3$) are used for smaller ResNets (18, 34).\n",
    "3.  **Bottleneck Blocks** ($1\\times1 \\to 3\\times3 \\to 1\\times1$) are used for deeper ResNets (50+) to save computation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1-3: Scaling Up â€“ Batch Training Deep Networks on HPC\n",
    "\n",
    "**Course:** CSEN 342: Deep Learning  \n",
    "**Topic:** HPC Workflows and Deep Feed-Forward Networks\n",
    "\n",
    "## Objective\n",
    "In this lab, you will transition from training simple models interactively to submitting computational jobs to a cluster. You will design a Deep Feed-Forward Neural Network to classify images from the **Fashion-MNIST** dataset. \n",
    "\n",
    "You will learn to:\n",
    "1. Define a Multi-Layer Perceptron (MLP) with non-linear activation functions.\n",
    "2. Write a standalone training script that can run headless on a GPU node.\n",
    "3. Submit a training job using the Slurm Workload Manager.\n",
    "4. Analyze training and validation loss curves to identify model convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Exploration & Model Design (Interactive)\n",
    "\n",
    "Before we submit a job to the cluster, we must ensure our data loading pipeline works and our model architecture is valid. We will use the **Fashion-MNIST** dataset, which consists of 28x28 grayscale images of 10 fashion categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Import utility functions\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "from utils import download_fashion_mnist\n",
    "\n",
    "# Check if GPU is available in the current notebook session\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Notebook session device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load and Visualize Data\n",
    "We define a transform to normalize the image data. Normalization helps the neural network learn faster and more stably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations: Convert to Tensor and Normalize to range [-1, 1]\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Download and load training data\n",
    "download_fashion_mnist()\n",
    "trainset = torchvision.datasets.FashionMNIST(root='../data', train=True, download=False, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Define classes\n",
    "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot')\n",
    "\n",
    "# Function to show an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# Get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Show images\n",
    "print(\"Sample training images:\")\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# Print labels\n",
    "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Define the Neural Network\n",
    "We will build a network with two hidden layers. We use **ReLU** (Rectified Linear Unit) activation functions to introduce non-linearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FashionMLP, self).__init__()\n",
    "        # Flatten input: 28x28 images -> 784 vector\n",
    "        self.flatten = nn.Flatten()\n",
    "        # Hidden Layer 1: 784 inputs -> 512 neurons\n",
    "        self.fc1 = nn.Linear(28 * 28, 512)\n",
    "        self.relu = nn.ReLU()\n",
    "        # Hidden Layer 2: 512 inputs -> 256 neurons\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        # Output Layer: 256 inputs -> 10 class scores\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        logits = self.fc3(x)\n",
    "        return logits\n",
    "\n",
    "# Sanity Check: Instantiate model and pass dummy data\n",
    "model = FashionMLP()\n",
    "outputs = model(images)\n",
    "print(\"Model architecture verified.\")\n",
    "print(\"Input shape:\", images.shape)\n",
    "print(\"Output shape:\", outputs.shape) # Should be [4, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: The HPC Workflow (Batch Submission)\n",
    "\n",
    "Training deep networks can take hours or days. Instead of keeping this notebook open, we will create a standalone Python script and submit it to the cluster scheduler (Slurm).\n",
    "\n",
    "### 2.1 Create the Training Script\n",
    "We use the `%%writefile` magic command to save the code below into a file named `train.py`. This script includes:\n",
    "1. Device configuration (GPU detection).\n",
    "2. Data loading.\n",
    "3. The training loop.\n",
    "4. **Logging:** It saves training and validation loss to a CSV file so we can plot it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# Import utility functions\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "from utils import download_fashion_mnist\n",
    "\n",
    "# 1. Device Configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Training on device: {device}\")\n",
    "\n",
    "# 2. Hyperparameters & Data\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 15\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Load Datasets\n",
    "download_fashion_mnist()\n",
    "full_trainset = torchvision.datasets.FashionMNIST(root='../data', train=True, download=False, transform=transform)\n",
    "# Split into Train (50k) and Validation (10k)\n",
    "train_set, val_set = torch.utils.data.random_split(full_trainset, [50000, 10000])\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "valloader = torch.utils.data.DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "# 3. Model Definition\n",
    "class FashionMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FashionMLP, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(28*28, 512)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "model = FashionMLP().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# 4. Training Loop with CSV Logging\n",
    "log_filename = 'training_log.csv'\n",
    "with open(log_filename, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    # Write header\n",
    "    writer.writerow(['epoch', 'train_loss', 'val_loss', 'val_acc'])\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        # Training Phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(trainloader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = running_loss / len(trainloader)\n",
    "\n",
    "        # Validation Phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in valloader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(valloader)\n",
    "        val_acc = 100 * correct / total\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Acc: {val_acc:.2f}%\")\n",
    "        \n",
    "        # Write stats to CSV\n",
    "        writer.writerow([epoch+1, avg_train_loss, avg_val_loss, val_acc])\n",
    "\n",
    "    print(f\"Training finished in {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "# Save the trained model weights\n",
    "torch.save(model.state_dict(), 'fashion_mlp.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Create the Slurm Submission Script\n",
    "Next, we create the shell script that tells the scheduler what resources we need. \n",
    "**Note:** We are requesting 1 GPU (`--gres=gpu:1`), but the `hub` partition does not allow that parameter (it automatically provides access to the GPU on the system). Make sure to change the user email to your email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile submit.sh\n",
    "#!/bin/bash\n",
    "#SBATCH --partition=hub                # Partition (queue) name\n",
    "#SBATCH --job-name=fashion_train\n",
    "#SBATCH --output=fashion_train_%j.log  # Standard output log\n",
    "#SBATCH --error=fashion_train_%j.err   # Standard error log\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=4\n",
    "## #SBATCH --gres=gpu:1                   # Request 1 GPU (not valid for hub partition, so commented out)\n",
    "#SBATCH --time=00:15:00                # Time limit\n",
    "#SBATCH --mem=8G\n",
    "#SBATCH --mail-user=user_account@scu.edu\n",
    "#SBATCH --mail-type=END\n",
    "\n",
    "# Load necessary modules (Uncomment and adjust based on your cluster's specific modules)\n",
    "module load Anaconda3\n",
    "conda activate 342wi26\n",
    "\n",
    "# Execute the python script\n",
    "python train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Submit the Job\n",
    "We use the `sbatch` command to submit the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sbatch submit.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Monitor Job Status\n",
    "Run the cell below to check the status of your job. \n",
    "* `PD`: Pending (waiting for resources)\n",
    "* `R`: Running\n",
    "* `CG`: Completing\n",
    "* (Empty): Job has finished\n",
    "\n",
    "Re-run this cell until the job is no longer listed, or wait for the email that the job has completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!squeue -u $USER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Analysis and Visualization\n",
    "\n",
    "Once the job has finished, it will have created `training_log.csv`. We will load this file to visualize the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "log_file = 'training_log.csv'\n",
    "\n",
    "if os.path.exists(log_file):\n",
    "    # Load the logs generated by the batch job\n",
    "    df = pd.read_csv(log_file)\n",
    "\n",
    "    # Create a plot with two lines: Training Loss and Validation Loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    plt.plot(df['epoch'], df['train_loss'], label='Training Loss', marker='o', linestyle='-', color='blue')\n",
    "    plt.plot(df['epoch'], df['val_loss'], label='Validation Loss', marker='x', linestyle='--', color='red')\n",
    "\n",
    "    plt.title('Learning Curves: Training vs. Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Display final accuracy\n",
    "    print(f\"Final Validation Accuracy: {df['val_acc'].iloc[-1]:.2f}%\")\n",
    "else:\n",
    "    print(\"Log file not found. The job might still be running or encountered an error.\")\n",
    "    print(\"Check the .err file generated by Slurm for details.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "1. **Convergence:** Look at the plot. At what epoch does the validation loss stop decreasing? This is arguably where you should stop training to prevent wasted resources.\n",
    "2. **Overfitting:** Do you see the Training Loss continue to go down while the Validation Loss starts to go up (or flatten out)? If so, your model is starting to memorize the training data rather than generalizing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "342wi26",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 8-2: The Ghost of Shakespeare â€“ \"Character-Level RNN\"\n",
    "\n",
    "**Course:** CSEN 342: Deep Learning  \n",
    "**Topic:** Recurrent Neural Networks (RNNs), Sequence Modeling, and Text Generation\n",
    "\n",
    "## Objective\n",
    "In the lecture (Slide 53), we introduced the **Vanilla RNN** equation:\n",
    "$$ h_t = \\tanh(W_{hh}h_{t-1} + W_{xh}x_t) $$\n",
    "\n",
    "Most deep learning libraries provide a black-box `nn.RNN` layer that hides this logic. In this tutorial, we will **open the black box**. \n",
    "\n",
    "We will:\n",
    "1.  **Implement `RNNCell` from scratch:** You will write the raw matrix multiplications and activation functions.\n",
    "2.  **Unroll the Network:** You will write the loop that passes the \"hidden state\" memory from one step to the next.\n",
    "3.  **Train on Shakespeare:** We will teach the network to generate text that looks like (gibberish) Shakespeare.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Preparation\n",
    "\n",
    "We treat text as a sequence of characters. Our goal is: given a sequence of characters (e.g., \"hell\"), predict the next character (\"o\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# 1. Download Data (Tiny Shakespeare)\n",
    "data_root = '../data'\n",
    "os.makedirs(data_root, exist_ok=True)\n",
    "file_path = os.path.join(data_root, 'tinyshakespeare.txt')\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    print(\"Downloading Tiny Shakespeare...\")\n",
    "    os.system(f\"wget -nc -P {data_root} https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\")\n",
    "    os.rename(os.path.join(data_root, 'input.txt'), file_path)\n",
    "\n",
    "# 2. Load and Tokenize\n",
    "with open(file_path, 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Mappings\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "print(f\"Total characters: {len(text):,}\")\n",
    "print(f\"Unique characters (Vocab size): {vocab_size}\")\n",
    "print(f\"First 100 chars:\\n{text[:100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: The Raw RNN Cell\n",
    "\n",
    "This is the heart of the tutorial. We will implement the equation from **Slide 53** manually.\n",
    "\n",
    "We need two linear transformations:\n",
    "1.  `i2h`: Input to Hidden ($W_{xh} x_t$)\n",
    "2.  `h2h`: Hidden to Hidden ($W_{hh} h_{t-1}$)\n",
    "\n",
    "The combined state is passed through a `Tanh` activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaRNNCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # W_xh: Input -> Hidden\n",
    "        self.i2h = nn.Linear(input_size, hidden_size)\n",
    "        # W_hh: Hidden -> Hidden\n",
    "        self.h2h = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        # The Core Equation (Slide 53)\n",
    "        # h_t = tanh(W_xh * x_t + W_hh * h_{t-1})\n",
    "        \n",
    "        # 1. Compute contributions\n",
    "        from_input = self.i2h(x)\n",
    "        from_hidden = self.h2h(hidden)\n",
    "        \n",
    "        # 2. Combine and Activate\n",
    "        next_hidden = torch.tanh(from_input + from_hidden)\n",
    "        \n",
    "        return next_hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: The Unrolled Network\n",
    "\n",
    "An RNN cell only handles *one time step*. To process a sentence, we need a loop. This corresponds to the \"unrolled\" computational graph seen in **Slide 61**.\n",
    "\n",
    "**Architecture:**\n",
    "1.  **Embedding:** Converts char indices to vectors.\n",
    "2.  **RNN Loop:** Updates hidden state step-by-step.\n",
    "3.  **Output Layer:** Converts hidden state to vocabulary probabilities ($y_t = W_{hy} h_t$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # 1. Embedding Layer (Optional but recommended for better performance)\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # 2. Our Custom Cell\n",
    "        self.rnn_cell = VanillaRNNCell(embedding_dim, hidden_size)\n",
    "        \n",
    "        # 3. Output Layer (Hidden -> Vocab Class scores)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, input_seq, hidden=None):\n",
    "        # input_seq shape: (Batch, Seq_Len)\n",
    "        batch_size, seq_len = input_seq.size()\n",
    "        \n",
    "        if hidden is None:\n",
    "            hidden = self.init_hidden(batch_size)\n",
    "            \n",
    "        # Convert indices to vectors: (Batch, Seq, Emb_Dim)\n",
    "        embeds = self.embedding(input_seq)\n",
    "        \n",
    "        outputs = []\n",
    "        \n",
    "        # --- The Recurrent Loop (Slide 61) ---\n",
    "        for t in range(seq_len):\n",
    "            # Extract input at time t\n",
    "            x_t = embeds[:, t, :] \n",
    "            \n",
    "            # Update hidden state using our custom cell\n",
    "            hidden = self.rnn_cell(x_t, hidden)\n",
    "            \n",
    "            # Compute output y_t for this step (Many-to-Many architecture)\n",
    "            out_t = self.fc(hidden)\n",
    "            outputs.append(out_t)\n",
    "            \n",
    "        # Stack outputs to shape (Batch, Seq_Len, Vocab_Size)\n",
    "        return torch.stack(outputs, dim=1), hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(batch_size, self.hidden_size).to(next(self.parameters()).device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Training Loop\n",
    "\n",
    "We treat this as a standard classification problem. At every time step $t$, we try to predict the character at $t+1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "hidden_size = 128\n",
    "embedding_dim = 64\n",
    "seq_len = 50\n",
    "batch_size = 64\n",
    "lr = 0.005\n",
    "epochs = 2000 # Iterations, not full epochs for this tutorial speed\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = CharRNN(vocab_size, hidden_size, embedding_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Helper to get random batch\n",
    "def get_batch(text_data, seq_len, batch_size):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for _ in range(batch_size):\n",
    "        start_idx = np.random.randint(0, len(text_data) - seq_len - 1)\n",
    "        chunk = text_data[start_idx : start_idx + seq_len + 1]\n",
    "        # Convert to indices\n",
    "        indices = [char_to_idx[c] for c in chunk]\n",
    "        inputs.append(indices[:-1])   # 0 to 49\n",
    "        targets.append(indices[1:])   # 1 to 50\n",
    "        \n",
    "    return torch.tensor(inputs).to(device), torch.tensor(targets).to(device)\n",
    "\n",
    "print(\"Starting Training...\")\n",
    "losses = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    inputs, targets = get_batch(text, seq_len, batch_size)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    # Note: We detach hidden state (Truncated BPTT) implicitly by re-initializing zeros each batch in this simple loop\n",
    "    outputs, _ = model(inputs)\n",
    "    \n",
    "    # Flatten outputs for CrossEntropy: (Batch*Seq, Vocab)\n",
    "    loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
    "    loss.backward()\n",
    "    \n",
    "    # Gradient Clipping (Slide 80 - Critical for Vanilla RNNs!)\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        losses.append(loss.item())\n",
    "        print(f\"Iter {i} Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"Training Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Generation (Dreaming)\n",
    "\n",
    "Now the fun part. We can seed the model with a character (e.g., 'T') and ask it to predict the next one. We then feed that prediction back in as input.\n",
    "\n",
    "**Sampling:** Instead of just taking the `argmax` (which is boring and repetitive), we sample from the probability distribution. This introduces variety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, start_str=\"The\", predict_len=200, temperature=0.8):\n",
    "    model.eval()\n",
    "    hidden = None\n",
    "    \n",
    "    # Convert start string to indices\n",
    "    input_indices = [char_to_idx[c] for c in start_str]\n",
    "    input_tensor = torch.tensor(input_indices).unsqueeze(0).to(device)\n",
    "    \n",
    "    # \"Prime\" the network (build up hidden state context)\n",
    "    # We only care about the hidden state, not the outputs here\n",
    "    with torch.no_grad():\n",
    "        _, hidden = model(input_tensor, hidden)\n",
    "    \n",
    "    # Use the last character as the first input for generation\n",
    "    current_input = input_tensor[:, -1].unsqueeze(1)\n",
    "    \n",
    "    generated_str = start_str\n",
    "    \n",
    "    for _ in range(predict_len):\n",
    "        with torch.no_grad():\n",
    "            # Single step forward\n",
    "            output, hidden = model(current_input, hidden)\n",
    "            \n",
    "            # Output is (Batch=1, Seq=1, Vocab)\n",
    "            logits = output.squeeze() \n",
    "            \n",
    "            # Apply Temperature (Higher = crazier, Lower = safer)\n",
    "            probs = torch.softmax(logits / temperature, dim=0)\n",
    "            \n",
    "            # Sample from distribution\n",
    "            char_idx = torch.multinomial(probs, 1).item()\n",
    "            \n",
    "            # Append to string\n",
    "            generated_str += idx_to_char[char_idx]\n",
    "            \n",
    "            # Prepare input for next step\n",
    "            current_input = torch.tensor([[char_idx]]).to(device)\n",
    "            \n",
    "    return generated_str\n",
    "\n",
    "print(\"--- Generated Text (Temperature 0.8) ---\")\n",
    "print(generate(model, start_str=\"ROMEO:\", temperature=0.8))\n",
    "\n",
    "print(\"\\n--- Generated Text (Temperature 0.5 - Safer) ---\")\n",
    "print(generate(model, start_str=\"ROMEO:\", temperature=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "You likely see text that *looks* like a play (CAPITAL names, newlines, maybe some old English words like \"thee\" or \"thou\"), even if it doesn't make total sense.\n",
    "\n",
    "**Why this is amazing:**\n",
    "We never taught the model what a \"word\" is, or how to spell, or grammar rules. It learned all of this purely by observing the statistical probability of character $B$ following character $A$.\n",
    "\n",
    "**Limitations of Vanilla RNN:**\n",
    "If you train this longer, you might notice it struggles to keep track of long-term context (e.g., closing a parenthesis opened 100 characters ago). This is the **Vanishing Gradient** problem (Slide 78), which LSTMs (Tutorial 22) are designed to solve."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

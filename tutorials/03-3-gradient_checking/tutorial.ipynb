{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3-3: Trust but Verify â€“ \"Gradient Checking\"\n",
    "\n",
    "**Course:** CSEN 342: Deep Learning  \n",
    "**Topic:** Numerical Differentiation, Gradient Checks, and Debugging Backprop\n",
    "\n",
    "## Objective\n",
    "Implementing backpropagation from scratch is prone to subtle bugs (e.g., off-by-one errors, sign flips). Even if your model trains, the gradients might be slightly wrong, leading to poor convergence.\n",
    "\n",
    "In this tutorial, first we will learn how to **verify** our gradients using **Finite Difference Approximation**. We will:\n",
    "1.  **Implement Numerical Gradients:** Write a function to compute the \"Two-sided derivative\" approximation: $\\frac{f(x+\\epsilon) - f(x-\\epsilon)}{2\\epsilon}$.\n",
    "2.  **The Bug Hunt:** You will be given a broken manual backprop implementation. You must use your gradient checker to identify which layer has the bug.\n",
    "3.  **Fix and Verify:** Correct the math and confirm that the difference between analytic and numerical gradients drops below $10^{-7}$.\n",
    "\n",
    "However, in the real world you will not be writing python/numpy code for training your network. You will use Pytorch for that. While PyTorch provides a vast library of standard operations, advanced research often requires implementing **custom activation functions**, loss functions, or layers that don't exist yet.\n",
    "\n",
    "To do this, you must tell PyTorch exactly how to calculate the gradient (the `backward` pass). If your math is even slightly off, the model might train but converge to a suboptimal solution. We will:\n",
    "\n",
    "4.  **Build a Custom Op:** Implement a custom **Sigmoid** activation function by subclassing `torch.autograd.Function`.\n",
    "5.  **Introduce a Bug:** We will intentionally make a mistake in the derivative calculation.\n",
    "6.  **Verify with `gradcheck`:** Use PyTorch's built-in utility to mathematically verify our implementation against a finite-difference approximation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Numerical Differentiation\n",
    "\n",
    "Calculus tells us that the derivative is the slope of the tangent line. We can approximate this slope by evaluating the function at two very close points ($x+\\epsilon$ and $x-\\epsilon$).\n",
    "\n",
    "$$ \\frac{df}{dx} \\approx \\frac{f(x+\\epsilon) - f(x-\\epsilon)}{2\\epsilon} $$\n",
    "\n",
    "This is slow (requires two forward passes for *every* parameter), but it is the \"ground truth\" we use to check our fast backprop code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical Gradient Checker Defined.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def eval_numerical_gradient(f, x, verbose=True, h=0.00001):\n",
    "    \"\"\" \n",
    "    a naive implementation of numerical gradient of f at x \n",
    "    - f should be a function that takes a single argument\n",
    "    - x is the point (numpy array) to evaluate the gradient at\n",
    "    \"\"\"\n",
    "    fx = f(x) # evaluate function value at original point\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    # Iterate over all indexes in x\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "        oldval = x[ix]\n",
    "        \n",
    "        # Calculate f(x + h)\n",
    "        x[ix] = oldval + h\n",
    "        fxph = f(x)\n",
    "        \n",
    "        # Calculate f(x - h)\n",
    "        x[ix] = oldval - h\n",
    "        fxmh = f(x)\n",
    "        \n",
    "        # Restore value\n",
    "        x[ix] = oldval \n",
    "        \n",
    "        # Compute the partial derivative with centered formula\n",
    "        grad[ix] = (fxph - fxmh) / (2 * h) \n",
    "        \n",
    "        if verbose:\n",
    "            print(ix, grad[ix])\n",
    "        it.iternext() \n",
    "\n",
    "    return grad\n",
    "\n",
    "print(\"Numerical Gradient Checker Defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Simple Test\n",
    "Let's verify our checker on a simple function: $f(x) = x^2$. The derivative should be $2x$. \n",
    "At $x=5$, we expect the gradient to be $10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0,) 9.999999999621423\n",
      "Gradient at x=5: 9.999999999621423 (Expected: 10.0)\n"
     ]
    }
   ],
   "source": [
    "def square_func(x):\n",
    "    return np.sum(x**2)\n",
    "\n",
    "x = np.array([5.0])\n",
    "grad = eval_numerical_gradient(square_func, x)\n",
    "print(f\"Gradient at x=5: {grad[0]} (Expected: 10.0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: The Bug Hunt\n",
    "\n",
    "Below is a class `BrokenTwoLayerNet` that implements a simple neural network. However, **one of the backpropagation lines has a math error**.\n",
    "\n",
    "Your job is to use `eval_numerical_gradient` to find which parameter (W1, b1, W2, or b2) has the incorrect gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Broken Model Defined.\n"
     ]
    }
   ],
   "source": [
    "class BrokenTwoLayerNet(object):\n",
    "    def __init__(self, input_size, hidden_size, output_size, std=1e-4):\n",
    "        self.params = {}\n",
    "        self.params['W1'] = std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    def loss(self, X, y=None):\n",
    "        W1, b1 = self.params['W1'], self.params['b1']\n",
    "        W2, b2 = self.params['W2'], self.params['b2']\n",
    "        N, D = X.shape\n",
    "\n",
    "        # --- Forward Pass ---\n",
    "        # Layer 1\n",
    "        h1_score = X.dot(W1) + b1\n",
    "        h1 = np.maximum(0, h1_score) # ReLU\n",
    "        \n",
    "        # Layer 2\n",
    "        scores = h1.dot(W2) + b2\n",
    "        \n",
    "        if y is None:\n",
    "            return scores\n",
    "\n",
    "        # Softmax Loss\n",
    "        exp_scores = np.exp(scores)\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "        correct_logprobs = -np.log(probs[range(N), y])\n",
    "        loss = np.sum(correct_logprobs) / N\n",
    "        \n",
    "        # --- Backward Pass ---\n",
    "        grads = {}\n",
    "        \n",
    "        # dScores\n",
    "        dscores = probs\n",
    "        dscores[range(N), y] -= 1\n",
    "        dscores /= N\n",
    "\n",
    "        # Backprop W2, b2\n",
    "        grads['W2'] = h1.T.dot(dscores)\n",
    "        grads['b2'] = np.sum(dscores, axis=0)\n",
    "\n",
    "        # Backprop Hidden Layer\n",
    "        dhidden = dscores.dot(W2.T)\n",
    "        dhidden[h1 <= 0] = 0\n",
    "\n",
    "        # Backprop W1, b1\n",
    "        # BUG IS LIKELY HERE\n",
    "        grads['W1'] = X.T.dot(dhidden) * 2.0  # <--- Suspicious multiplication?\n",
    "        grads['b1'] = np.sum(dhidden, axis=0)\n",
    "\n",
    "        return loss, grads\n",
    "\n",
    "print(\"Broken Model Defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Running the Check\n",
    "\n",
    "We generate some dummy data and run the check. The `rel_error` function calculates the relative difference between two matrices. \n",
    "\n",
    "$$ \\text{rel\\_error} = \\frac{|x - y|}{\\max(|x| + |y|, 1e-8)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W2 max relative error: 6.37e-10\n",
      "b2 max relative error: 4.45e-11\n",
      "W1 max relative error: 3.33e-01\n",
      "b1 max relative error: 2.74e-09\n"
     ]
    }
   ],
   "source": [
    "def rel_error(x, y):\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "# Create tiny data\n",
    "input_size = 4\n",
    "hidden_size = 10\n",
    "num_classes = 3\n",
    "num_inputs = 5\n",
    "\n",
    "def init_toy_model():\n",
    "    np.random.seed(0)\n",
    "    return BrokenTwoLayerNet(input_size, hidden_size, num_classes, std=1e-1)\n",
    "\n",
    "def init_toy_data():\n",
    "    np.random.seed(1)\n",
    "    X = 10 * np.random.randn(num_inputs, input_size)\n",
    "    y = np.array([0, 1, 2, 2, 1])\n",
    "    return X, y\n",
    "\n",
    "net = init_toy_model()\n",
    "X, y = init_toy_data()\n",
    "\n",
    "# 1. Analytic Gradient (from our backprop code)\n",
    "loss, grads = net.loss(X, y)\n",
    "\n",
    "# 2. Numerical Gradient (Ground Truth)\n",
    "# We check each parameter separately\n",
    "for param_name in grads:\n",
    "    f = lambda w: net.loss(X, y)[0] # Wrapper to compute loss for current weights\n",
    "    param_grad_num = eval_numerical_gradient(f, net.params[param_name], verbose=False)\n",
    "    \n",
    "    error = rel_error(param_grad_num, grads[param_name])\n",
    "    print(f'{param_name} max relative error: {error:.2e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** \n",
    "You should see that `W2`, `b2`, and `b1` have very small errors (around $10^{-8}$ or less). \n",
    "However, `W1` has a huge error (likely between $0.1$ and $1.0$). This confirms the bug is in the `W1` calculation.\n",
    "\n",
    "**The Fix:** Look closely at the `grads['W1']` line in `BrokenTwoLayerNet`. Remove the incorrect `* 2.0` multiplier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: The Fixed Model\n",
    "\n",
    "Correct the code below and verify that all gradients pass the check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Fixed Model:\n",
      "W2 max relative error: 6.37e-10\n",
      "b2 max relative error: 4.45e-11\n",
      "W1 max relative error: 1.53e-09\n",
      "b1 max relative error: 2.74e-09\n"
     ]
    }
   ],
   "source": [
    "# Copy of the class with the fix applied\n",
    "class FixedTwoLayerNet(BrokenTwoLayerNet):\n",
    "    def loss(self, X, y=None):\n",
    "        # ... (Forward pass is same) ...\n",
    "        W1, b1 = self.params['W1'], self.params['b1']\n",
    "        W2, b2 = self.params['W2'], self.params['b2']\n",
    "        N, D = X.shape\n",
    "\n",
    "        # Forward\n",
    "        h1_score = X.dot(W1) + b1\n",
    "        h1 = np.maximum(0, h1_score)\n",
    "        scores = h1.dot(W2) + b2\n",
    "        \n",
    "        if y is None: return scores\n",
    "        \n",
    "        # Loss\n",
    "        exp_scores = np.exp(scores)\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "        correct_logprobs = -np.log(probs[range(N), y])\n",
    "        loss = np.sum(correct_logprobs) / N\n",
    "\n",
    "        # Backward\n",
    "        grads = {}\n",
    "        dscores = probs\n",
    "        dscores[range(N), y] -= 1\n",
    "        dscores /= N\n",
    "\n",
    "        grads['W2'] = h1.T.dot(dscores)\n",
    "        grads['b2'] = np.sum(dscores, axis=0)\n",
    "\n",
    "        dhidden = dscores.dot(W2.T)\n",
    "        dhidden[h1 <= 0] = 0\n",
    "\n",
    "        # FIXED LINE:\n",
    "        grads['W1'] = X.T.dot(dhidden) # Removed * 2.0\n",
    "        grads['b1'] = np.sum(dhidden, axis=0)\n",
    "\n",
    "        return loss, grads\n",
    "\n",
    "# Run Check Again\n",
    "print(\"Checking Fixed Model:\")\n",
    "net_fixed = FixedTwoLayerNet(input_size, hidden_size, num_classes, std=1e-1)\n",
    "net_fixed.params = net.params # Copy weights from broken net to keep test consistent\n",
    "\n",
    "loss, grads_fixed = net_fixed.loss(X, y)\n",
    "\n",
    "for param_name in grads_fixed:\n",
    "    f = lambda w: net_fixed.loss(X, y)[0]\n",
    "    param_grad_num = eval_numerical_gradient(f, net_fixed.params[param_name], verbose=False)\n",
    "    error = rel_error(param_grad_num, grads_fixed[param_name])\n",
    "    print(f'{param_name} max relative error: {error:.2e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Anatomy of a Custom Autograd Function\n",
    "\n",
    "To create a new operation in PyTorch, you must define a class that inherits from `torch.autograd.Function` and implements two static methods:\n",
    "\n",
    "1.  **`forward(ctx, input)`**: Computes the output. You also use `ctx.save_for_backward(input)` to cache data needed for the gradient later.\n",
    "2.  **`backward(ctx, grad_output)`**: Computes the gradient of the loss with respect to the input. It receives the `grad_output` (upstream gradient) and must return `grad_input` (local gradient * upstream gradient).\n",
    "\n",
    "### The Math: Sigmoid\n",
    "$$ f(x) = \\sigma(x) = \\frac{1}{1 + e^{-x}} $$\n",
    "\n",
    "**Derivative:**\n",
    "$$ \\frac{\\partial f}{\\partial x} = \\sigma(x) \\cdot (1 - \\sigma(x)) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BuggySigmoid defined.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Function\n",
    "\n",
    "class BuggySigmoid(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return\n",
    "        a Tensor containing the output. ctx is a context object that can be used\n",
    "        to stash information for backward computation.\n",
    "        \"\"\"\n",
    "        output = 1.0 / (1.0 + torch.exp(-input))\n",
    "        \n",
    "        # Save the output for the backward pass\n",
    "        # (Recall: sigmoid derivative is easier to calculate using the output)\n",
    "        ctx.save_for_backward(output)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        output, = ctx.saved_tensors\n",
    "        \n",
    "        # --- THE BUG IS HERE ---\n",
    "        # Correct Math: output * (1 - output)\n",
    "        # Our Math:     output * (1 + output)  <-- Oops!\n",
    "        grad_sigmoid = output * (1.0 + output)\n",
    "        \n",
    "        # Chain Rule: local_grad * upstream_grad\n",
    "        return grad_sigmoid * grad_output\n",
    "\n",
    "# Wrap it in an alias so we can apply it like a function\n",
    "buggy_sigmoid = BuggySigmoid.apply\n",
    "\n",
    "print(\"BuggySigmoid defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: The Check (Why validation matters)\n",
    "\n",
    "If we tried to train a neural network with this bug, it might actually \"work\" (the loss might decrease), but it would behave strangely or converge slowly. Visual inspection of code is hard. Numerical inspection is better.\n",
    "\n",
    "PyTorch provides `torch.autograd.gradcheck`. It:\n",
    "1.  Computes the analytical gradient using your `backward` method.\n",
    "2.  Computes the numerical gradient using finite differences ($f(x+\\epsilon) - f(x-\\epsilon)$).\n",
    "3.  Compares them and raises an error if they differ.\n",
    "\n",
    "**Note:** `gradcheck` requires inputs to be **double precision** (`float64`) for numerical stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running gradcheck on BuggySigmoid...\n",
      "\n",
      "Test FAILED! PyTorch caught the bug.\n",
      "Error details:\n",
      "Jacobian mismatch for output 0 with respect to input 0,\n",
      "numerical:tensor([[0.1957, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1723, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.1957, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1337, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.2496]], dtype=torch.float64)\n",
      "analytical:tensor([[1.2702, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2703, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.2702, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.5484, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.7085]], dtype=torch.float64)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import gradcheck\n",
    "\n",
    "# 1. Create input data\n",
    "# We must enable requires_grad=True so PyTorch tracks it\n",
    "# We use double precision (float64) to minimize floating point errors during the check\n",
    "input_test = torch.randn(5, requires_grad=True, dtype=torch.float64)\n",
    "\n",
    "print(\"Running gradcheck on BuggySigmoid...\")\n",
    "\n",
    "try:\n",
    "    # gradcheck takes (function, inputs, eps=...)\n",
    "    # If the check passes, it returns True. If it fails, it raises a RuntimeError.\n",
    "    test = gradcheck(buggy_sigmoid, input_test, eps=1e-6, atol=1e-4)\n",
    "    print(\"Test Passed!\")\n",
    "except RuntimeError as e:\n",
    "    print(\"\\nTest FAILED! PyTorch caught the bug.\")\n",
    "    print(\"Error details:\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "You should see a failure message. PyTorch compares the **Numerical** Jacobian (calculated by perturbing inputs) with the **Analytical** Jacobian (calculated by your buggy code). Because we used `(1 + output)` instead of `(1 - output)`, the numbers will be wildly different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: The Fix\n",
    "\n",
    "Now, implement the class correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running gradcheck on CorrectSigmoid...\n",
      "Test Passed? True\n"
     ]
    }
   ],
   "source": [
    "class CorrectSigmoid(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        output = 1.0 / (1.0 + torch.exp(-input))\n",
    "        ctx.save_for_backward(output)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output, = ctx.saved_tensors\n",
    "        \n",
    "        # --- FIXED LOGIC ---\n",
    "        grad_sigmoid = output * (1.0 - output)\n",
    "        \n",
    "        return grad_sigmoid * grad_output\n",
    "\n",
    "correct_sigmoid = CorrectSigmoid.apply\n",
    "\n",
    "print(\"Running gradcheck on CorrectSigmoid...\")\n",
    "try:\n",
    "    test = gradcheck(correct_sigmoid, input_test, eps=1e-6, atol=1e-4)\n",
    "    print(f\"Test Passed? {test}\")\n",
    "except RuntimeError as e:\n",
    "    print(\"Test FAILED!\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once verified, you can use `CorrectSigmoid.apply` just like any other PyTorch function inside a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Create a custom module wrapper\n",
    "class MySigmoidLayer(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return CorrectSigmoid.apply(x)\n",
    "\n",
    "# Define a tiny model using our custom activation\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 5),\n",
    "    MySigmoidLayer(),  # Using our custom autograd function\n",
    "    nn.Linear(5, 1)\n",
    ")\n",
    "\n",
    "print(model)\n",
    "\n",
    "# Verify we can backprop through it\n",
    "dummy_input = torch.randn(2, 10)\n",
    "output = model(dummy_input)\n",
    "loss = output.mean()\n",
    "loss.backward()\n",
    "\n",
    "print(\"\\nBackward pass successful. Gradients computed for first layer:\")\n",
    "print(model[0].weight.grad[0][:5]) # Print a few gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "Gradient Checking is a powerful tool. When you are writing custom CUDA kernels or new layer types (where you can't rely on PyTorch `autograd`), this is the primary way to verify your derivative calculus is correct. Similarly, you need to check your gradients when you  **extend PyTorch** by writing custom `Function` subclasses. Whenever you implement complex math operations not supported by the standard library, **always** run a gradient check before training!\n",
    "\n",
    "**Note:** Numerical gradient checking is very slow. You should only use it for debugging on small networks, never for actual training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "342wi26",
   "language": "python",
   "name": "342wi26"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

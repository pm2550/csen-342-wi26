{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1-1: The Perceptron & The XOR Problem\n",
    "\n",
    "**Course:** CSEN 342: Deep Learning  \n",
    "**Topic:** Linear Classifiers, The Need for Hidden Layers, and Manual Backpropagation\n",
    "\n",
    "## Objective\n",
    "In this tutorial, we will explore the foundational concepts of neural networks by implementing them from scratch using **NumPy**. We will:\n",
    "1.  Implement the **Perceptron** algorithm and demonstrate its failure on the **XOR** problem.\n",
    "2.  Build a **Multi-Layer Perceptron (MLP)** from scratch using NumPy matrices.\n",
    "3.  Manually implement **Forward Propagation** and **Backpropagation** to solve XOR.\n",
    "\n",
    "This \"under the hood\" implementation will help you understand exactly what deep learning frameworks like PyTorch do for you automatically.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The Perceptron Limitations (Recap)\n",
    "\n",
    "We start by implementing a simple Perceptron to verify it works on linearly separable data (AND gate) but fails on non-linear data (XOR gate).\n",
    "\n",
    "**The Update Rule:**\n",
    "$$ \\theta^{[k+1]} = \\theta^{[k]} + \\lambda (y^{(i)} - h_{\\theta^{[k]}}(x^{(i)})) x^{(i)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class SimplePerceptron:\n",
    "    def __init__(self, input_size, lr=0.1, epochs=20):\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.weights = np.zeros(input_size + 1) # +1 for bias\n",
    "\n",
    "    def predict(self, x):\n",
    "        z = self.weights[0] + np.dot(x, self.weights[1:])\n",
    "        return 1 if z >= 0 else 0\n",
    "\n",
    "    def train(self, X, y):\n",
    "        self.errors_ = []\n",
    "        for _ in range(self.epochs):\n",
    "            errors = 0\n",
    "            for xi, target in zip(X, y):\n",
    "                update = self.lr * (target - self.predict(xi))\n",
    "                self.weights[1:] += update * xi\n",
    "                self.weights[0] += update\n",
    "                errors += int(update != 0.0)\n",
    "            self.errors_.append(errors)\n",
    "        return self\n",
    "\n",
    "# Visualization Utility\n",
    "def plot_decision_boundary(X, y, classifier, title):\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                         np.arange(y_min, y_max, 0.02))\n",
    "    \n",
    "    Z = np.array([classifier.predict(np.array([x, y_val])) for x, y_val in np.c_[xx.ravel(), yy.ravel()]])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.Paired)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', marker='o', s=100, cmap=plt.cm.Paired)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# 1. Define XOR Data\n",
    "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_xor = np.array([0, 1, 1, 0])\n",
    "\n",
    "# 2. Train Perceptron\n",
    "ppn = SimplePerceptron(input_size=2, lr=0.1, epochs=20)\n",
    "ppn.train(X_xor, y_xor)\n",
    "\n",
    "# 3. Visual Proof of Failure\n",
    "print(f\"Final Errors: {ppn.errors_[-1]}\")\n",
    "plot_decision_boundary(X_xor, y_xor, ppn, \"Perceptron Failure on XOR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: The Multi-Layer Perceptron (NumPy Implementation)\n",
    "\n",
    "To solve XOR, we need a hidden layer to transform the inputs into a linearly separable space. We will implement a network with the following architecture:\n",
    "\n",
    "1.  **Input Layer:** 2 Neurons ($x_1, x_2$)\n",
    "2.  **Hidden Layer:** 4 Neurons (Sigmoid Activation)\n",
    "3.  **Output Layer:** 1 Neuron (Sigmoid Activation)\n",
    "\n",
    "We will manually calculate the gradients for backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Activation Functions\n",
    "We need a non-linear activation function. We will use the **Sigmoid** function:\n",
    "$$ \\sigma(z) = \\frac{1}{1 + e^{-z}} $$\n",
    "\n",
    "For backpropagation, we also need its derivative:\n",
    "$$ \\sigma'(z) = \\sigma(z) \\cdot (1 - \\sigma(z)) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(output):\n",
    "    # output is already sigmoid(z)\n",
    "    return output * (1 - output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 The Manual MLP Class\n",
    "Here we implement the full training cycle:\n",
    "1.  **Init:** Random weights for Input->Hidden ($W1$) and Hidden->Output ($W2$).\n",
    "2.  **Forward:** Compute activations layer by layer.\n",
    "3.  **Backward:** Calculate error, compute deltas using the Chain Rule, and update weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyMLP:\n",
    "    def __init__(self, input_size, hidden_size, output_size, lr=0.1):\n",
    "        self.lr = lr\n",
    "        \n",
    "        # Initialize Weights (Input -> Hidden)\n",
    "        # Shape: (2 inputs, 4 hidden)\n",
    "        self.W1 = np.random.uniform(-1, 1, (input_size, hidden_size))\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        \n",
    "        # Initialize Weights (Hidden -> Output)\n",
    "        # Shape: (4 hidden, 1 output)\n",
    "        self.W2 = np.random.uniform(-1, 1, (hidden_size, output_size))\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "\n",
    "    def forward(self, X):\n",
    "        # 1. Input -> Hidden\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.a1 = sigmoid(self.z1)\n",
    "        \n",
    "        # 2. Hidden -> Output\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.a2 = sigmoid(self.z2)\n",
    "        return self.a2\n",
    "\n",
    "    def train(self, X, y, epochs=10000):\n",
    "        self.losses = []\n",
    "        y = y.reshape(-1, 1) # Ensure target is column vector\n",
    "        \n",
    "        for _ in range(epochs):\n",
    "            # --- Forward Pass ---\n",
    "            output = self.forward(X)\n",
    "            \n",
    "            # --- Backward Pass (Backpropagation) ---\n",
    "            # 1. Output Layer Error\n",
    "            # Error = (Target - Output)\n",
    "            error_output = y - output\n",
    "            \n",
    "            # Delta Output = Error * Derivative(Activation)\n",
    "            delta_output = error_output * sigmoid_derivative(output)\n",
    "            \n",
    "            # 2. Hidden Layer Error\n",
    "            # Propagate delta backwards: delta_output * W2_transpose\n",
    "            error_hidden = delta_output.dot(self.W2.T)\n",
    "            \n",
    "            # Delta Hidden = Error_Hidden * Derivative(Activation)\n",
    "            delta_hidden = error_hidden * sigmoid_derivative(self.a1)\n",
    "            \n",
    "            # --- Weight Updates (Gradient Descent) ---\n",
    "            # Update W2: Hidden_Output.T dot Delta_Output\n",
    "            self.W2 += self.a1.T.dot(delta_output) * self.lr\n",
    "            self.b2 += np.sum(delta_output, axis=0, keepdims=True) * self.lr\n",
    "            \n",
    "            # Update W1: Input.T dot Delta_Hidden\n",
    "            self.W1 += X.T.dot(delta_hidden) * self.lr\n",
    "            self.b1 += np.sum(delta_hidden, axis=0, keepdims=True) * self.lr\n",
    "            \n",
    "            # Track Mean Squared Error\n",
    "            loss = np.mean(np.square(error_output))\n",
    "            self.losses.append(loss)\n",
    "\n",
    "    def predict(self, x):\n",
    "        # Helper for the plotting function (thresholds probability at 0.5)\n",
    "        # Ensure input is 2D array\n",
    "        if x.ndim == 1:\n",
    "            x = x.reshape(1, -1)\n",
    "        return 1 if self.forward(x) > 0.5 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Training on XOR\n",
    "Now we train our manual NumPy network. We use 4 hidden neurons and run for 10,000 epochs to ensure convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Model: 2 Inputs -> 4 Hidden -> 1 Output\n",
    "mlp = NumpyMLP(input_size=2, hidden_size=4, output_size=1, lr=0.1)\n",
    "\n",
    "# Train\n",
    "mlp.train(X_xor, y_xor, epochs=10000)\n",
    "\n",
    "# Check Results\n",
    "print(\"Final Predictions:\")\n",
    "print(mlp.forward(X_xor).round(3))\n",
    "\n",
    "# Plot Loss Curve\n",
    "plt.plot(mlp.losses)\n",
    "plt.title(\"Training Loss (MSE)\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Mean Squared Error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Visualizing the Solution\n",
    "Because we introduced a hidden layer, the network can now learn a non-linear boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(X_xor, y_xor, mlp, \"NumPy MLP Decision Boundary (XOR)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Success!** The manually implemented Multi-Layer Perceptron has successfully separated the XOR classes. The hidden layer mapped the inputs to a space where the output layer could draw a line separating them.\n",
    "\n",
    "In the next tutorial, we will see how **PyTorch** automates all of this (forward pass, backward pass, and updates) so we don't have to write matrix math manually."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "342wi26",
   "language": "python",
   "name": "342wi26"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1-2: Breaking Linearity â€“ Intro to PyTorch & GPUs\n",
    "\n",
    "**Course:** CSEN 342: Deep Learning  \n",
    "**Topic:** PyTorch Basics, Multi-Layer Perceptrons, and GPU Acceleration\n",
    "\n",
    "## Objective\n",
    "In the previous tutorial, we manually implemented a neural network using NumPy to solve the XOR problem. In this tutorial, we will upgrade our workflow to use **PyTorch**, the industry-standard framework for Deep Learning.\n",
    "\n",
    "We will:\n",
    "1.  Re-implement the XOR classifier using PyTorch's `nn.Module` and autograd system.\n",
    "2.  Learn how to manage compute resources (CPU vs. GPU).\n",
    "3.  **Benchmark** a large matrix multiplication to scientifically demonstrate the speed advantage of using High Performance Computing (HPC) GPUs over standard CPUs.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Solving XOR with PyTorch\n",
    "\n",
    "PyTorch handles the heavy lifting of backpropagation for us. Instead of manually calculating derivatives and chain rules (as we did in NumPy), we define the \"Forward Pass\" and let PyTorch calculate the gradients automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 1. Define Data (XOR Problem)\n",
    "# Note: We must convert data to PyTorch Tensors (Float32)\n",
    "X = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
    "y = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n",
    "\n",
    "print(f\"Input Shape: {X.shape}\")\n",
    "print(f\"Target Shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Define the Model Architecture\n",
    "We will create a class that inherits from `nn.Module`. We need:\n",
    "* **Hidden Layer:** Linear transformation from 2 inputs -> 4 neurons.\n",
    "* **Activation:** ReLU or Sigmoid (to break linearity).\n",
    "* **Output Layer:** Linear transformation from 4 neurons -> 1 output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XORModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(XORModel, self).__init__()\n",
    "        # Layer 1: 2 input features -> 4 hidden neurons\n",
    "        self.layer1 = nn.Linear(2, 4)\n",
    "        # Layer 2: 4 hidden neurons -> 1 output neuron\n",
    "        self.layer2 = nn.Linear(4, 1)\n",
    "        # Activations\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through layer 1, then activation\n",
    "        x = self.sigmoid(self.layer1(x))\n",
    "        # Pass result through layer 2, then activation (for probability)\n",
    "        x = self.sigmoid(self.layer2(x))\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = XORModel()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Training Loop\n",
    "Instead of manually updating `W -= lr * gradient`, we use an **Optimizer** (`SGD` or `Adam`) and a **Loss Function** (`BCELoss` for binary classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.1\n",
    "epochs = 5000\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "losses = []\n",
    "\n",
    "print(\"Starting Training...\")\n",
    "for epoch in range(epochs):\n",
    "    # 1. Forward Pass\n",
    "    predictions = model(X)\n",
    "    loss = criterion(predictions, y)\n",
    "    \n",
    "    # 2. Backward Pass (Autograd)\n",
    "    optimizer.zero_grad()  # Clear previous gradients\n",
    "    loss.backward()        # Calculate new gradients\n",
    "    optimizer.step()       # Update weights\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"Training Complete.\")\n",
    "\n",
    "# Visualize Loss\n",
    "plt.plot(losses)\n",
    "plt.title(\"Training Loss Curve\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Verify Results\n",
    "Did PyTorch succeed where the linear Perceptron failed? Let's verify predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # Round output to get 0 or 1 class predictions\n",
    "    preds = model(X).round()\n",
    "    print(\"True Labels:\\n\", y.numpy().flatten())\n",
    "    print(\"Model Preds:\\n\", preds.numpy().flatten())\n",
    "    \n",
    "    accuracy = (preds.eq(y).sum() / float(y.shape[0])).item()\n",
    "    print(f\"Accuracy: {accuracy * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: GPU Basics & Benchmarking\n",
    "\n",
    "The XOR problem is tiny; a CPU solves it instantly. However, deep learning involves matrix multiplications of millions of parameters. This is where GPUs (Graphics Processing Units) shine because they are designed for massive parallelism.\n",
    "\n",
    "### 2.1 Device Management\n",
    "In PyTorch, you must explicitly move your data and your model to the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA (NVIDIA GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"GPU Detected: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Device Count: {torch.cuda.device_count()}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"No GPU detected. Running on CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Benchmarking: CPU vs. GPU\n",
    "\n",
    "We will create two massive matrices (size 10,000 x 10,000) and multiply them. We will time how long this takes on the CPU versus the GPU.\n",
    "\n",
    "*Note: The first time you run a CUDA operation, there is a slight initialization overhead.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Matrix Size: 10,000 x 10,000\n",
    "SIZE = 10000\n",
    "\n",
    "print(f\"Creating random matrices of size {SIZE}x{SIZE}...\")\n",
    "a_cpu = torch.randn(SIZE, SIZE)\n",
    "b_cpu = torch.randn(SIZE, SIZE)\n",
    "\n",
    "# --- CPU Benchmark ---\n",
    "print(\"Starting CPU Matrix Multiplication... (This might take a while)\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Perform matrix multiplication on CPU\n",
    "c_cpu = torch.matmul(a_cpu, b_cpu)\n",
    "\n",
    "end_time = time.time()\n",
    "cpu_time = end_time - start_time\n",
    "print(f\"CPU Time: {cpu_time:.4f} seconds\")\n",
    "\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- GPU Benchmark ---\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Moving tensors to GPU...\")\n",
    "    # Move data to GPU\n",
    "    a_gpu = a_cpu.to(device)\n",
    "    b_gpu = b_cpu.to(device)\n",
    "    \n",
    "    # Warm-up (GPUs sometimes need a dummy op to initialize context)\n",
    "    _ = torch.matmul(a_gpu[:100, :100], b_gpu[:100, :100])\n",
    "    torch.cuda.synchronize() # Wait for warm-up to finish\n",
    "    \n",
    "    print(\"Starting GPU Matrix Multiplication...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Perform matrix multiplication on GPU\n",
    "    c_gpu = torch.matmul(a_gpu, b_gpu)\n",
    "    \n",
    "    # Important: CUDA operations are asynchronous.\n",
    "    # We must synchronize to ensure the calculation is actually done before stopping the timer.\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    gpu_time = end_time - start_time\n",
    "    print(f\"GPU Time: {gpu_time:.4f} seconds\")\n",
    "    \n",
    "    # Calculate Speedup\n",
    "    speedup = cpu_time / gpu_time\n",
    "    print(f\"\\nSpeedup Factor: {speedup:.2f}x faster on GPU\")\n",
    "else:\n",
    "    print(\"Skipping GPU test (No device found).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion Questions\n",
    "1.  **Speedup:** Look at the speedup factor. Why is the GPU so much faster for this specific operation?\n",
    "2.  **Overhead:** If we performed this multiplication on very small matrices (e.g., 5x5), do you think the GPU would still be faster? Why or why not? (Hint: Consider the time it takes to move data from RAM to GPU memory).\n",
    "3.  **Workflow:** In the code above, we moved data using `.to(device)`. If you forget this step and try to multiply a CPU tensor with a GPU tensor, what happens?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "342wi26",
   "language": "python",
   "name": "342wi26"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2-1: The Mechanics of Backprop – \"Autograd from Scratch\"\n",
    "\n",
    "**Course:** CSEN 342: Deep Learning  \n",
    "**Topic:** Computational Graphs, Chain Rule, and Automatic Differentiation\n",
    "\n",
    "## Objective\n",
    "Deep learning frameworks like PyTorch and TensorFlow rely on **Automatic Differentiation** (Autograd). They build a computational graph of your operations and automatically calculate gradients for you.\n",
    "\n",
    "In this tutorial, we will \"open the black box\" by building a tiny Autograd engine from scratch. We will:\n",
    "1.  Create a `Value` class that stores data and its gradient.\n",
    "2.  Manually implement the `backward()` pass for basic operations (Add, Multiply, ReLU, Sigmoid).\n",
    "3.  Recreate the detailed backpropagation example from the lecture slides to verify our math.\n",
    "4.  Compare our manual engine against PyTorch to see that they produce identical results.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The Engine (A Tiny `Tensor`)\n",
    "\n",
    "We need a data structure that can store a scalar value (the data) and the gradient of the loss with respect to that value (the derivative).\n",
    "\n",
    "Recall the Chain Rule from the slides:\n",
    "$$ \\text{upstream\\_grad} = \\frac{\\partial L}{\\partial \\text{output}} $$\n",
    "$$ \\text{local\\_grad} = \\frac{\\partial \\text{output}}{\\partial \\text{input}} $$\n",
    "$$ \\frac{\\partial L}{\\partial \\text{input}} = \\text{local\\_grad} * \\text{upstream\\_grad} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "class Value:\n",
    "    def __init__(self, data, _children=(), _op='', label=''):\n",
    "        self.data = data\n",
    "        self.grad = 0.0          # Initially, gradient is zero\n",
    "        self._backward = lambda: None # Function to propagate gradient backward\n",
    "        self._prev = set(_children)   # Keep track of input nodes (for the graph)\n",
    "        self._op = _op           # The operation that produced this node\n",
    "        self.label = label       # Optional name for debugging\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data:.4f}, grad={self.grad:.4f})\"\n",
    "\n",
    "    # --- Operations ---\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "\n",
    "        def _backward():\n",
    "            # Gradient of addition is 1.0 * upstream\n",
    "            # f = a + b  -> df/da = 1, df/db = 1\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "\n",
    "        def _backward():\n",
    "            # Gradient of mult is the other value * upstream\n",
    "            # f = a * b -> df/da = b, df/db = a\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    # Support for the power operator (**) needed for x^-1\\n\n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float)), \"Only supporting int/float powers for now\"\n",
    "        out = Value(self.data ** other, (self,), f'**{other}')\n",
    "\n",
    "        def _backward():\n",
    "            # f = x^n -> df/dx = n * x^(n-1)\n",
    "            self.grad += other * (self.data ** (other - 1)) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    # Re-using __mul__ for negation (-1 * self)\n",
    "    def __neg__(self):\n",
    "        return self * -1\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return self + (-other)\n",
    "\n",
    "    def sigmoid(self):\n",
    "        x = self.data\n",
    "        t = 1 / (1 + math.exp(-x))\n",
    "        out = Value(t, (self,), 'sigmoid')\n",
    "\n",
    "        def _backward():\n",
    "            # d(sigmoid)/dx = sigmoid * (1 - sigmoid)\n",
    "            local_grad = t * (1 - t)\n",
    "            self.grad += local_grad * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def exp(self):\n",
    "        x = self.data\n",
    "        out = Value(math.exp(x), (self,), 'exp')\n",
    "        \n",
    "        def _backward():\n",
    "            # d(e^x)/dx = e^x\n",
    "            self.grad += out.data * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def backward(self):\n",
    "        # Topological sort to ensure we backprop in the correct order\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "        \n",
    "        # Initialize gradient of the final output (loss) to 1.0\n",
    "        self.grad = 1.0\n",
    "        for node in reversed(topo):\n",
    "            node._backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Recreating the Lecture Slide Example\n",
    "\n",
    "Let's recreate the computational graph shown in the lecture slides (Slide 14-27). \n",
    "\n",
    "**The Function:**\n",
    "$$ f(x, w) = \\frac{1}{1 + e^{-(w_0x_0 + w_1x_1 + w_2)}} $$\n",
    "\n",
    "**The Inputs:**\n",
    "* $w_0 = 2.0, x_0 = -1.0$\n",
    "* $w_1 = -3.0, x_1 = -2.0$\n",
    "* $w_2 = -3.0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Initialize Inputs\n",
    "w0 = Value(2.0, label='w0'); x0 = Value(-1.0, label='x0')\n",
    "w1 = Value(-3.0, label='w1'); x1 = Value(-2.0, label='x1')\n",
    "w2 = Value(-3.0, label='w2')\n",
    "\n",
    "# 2. Forward Pass (Building the Graph)\n",
    "# Operations mimic the slide flow:\n",
    "dot1 = w0 * x0; dot1.label = 'w0*x0'\n",
    "dot2 = w1 * x1; dot2.label = 'w1*x1'\n",
    "\n",
    "sum1 = dot1 + dot2; sum1.label = 'sum_dots'\n",
    "sum2 = sum1 + w2;   sum2.label = 'sum_bias'\n",
    "\n",
    "# Slide implements sigmoid manually as: 1 / (1 + exp(-x))\n",
    "# We will do the same to match the slide's step-by-step derivative\n",
    "neg_z = -sum2;          neg_z.label = '-z'\n",
    "exp_z = neg_z.exp();    exp_z.label = 'exp(-z)'\n",
    "den = exp_z + 1.0;      den.label = '1 + exp(-z)'\n",
    "out = den**-1;          out.label = '1/x' \n",
    "# Note: Our Value class needs __pow__ for den**-1. \n",
    "# Let's add a quick power helper just for this demo:\n",
    "def power(self, other):\n",
    "    out = Value(self.data**other, (self,), f'**{other}')\n",
    "    def _backward():\n",
    "        self.grad += (other * self.data**(other-1)) * out.grad\n",
    "    out._backward = _backward\n",
    "    return out\n",
    "Value.__pow__ = power\n",
    "\n",
    "# Rerun the final step with the new power method\n",
    "out = den**-1; out.label = 'output'\n",
    "\n",
    "print(\"Forward Pass Complete.\")\n",
    "print(\"Output:\", out.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Backward Pass\n",
    "Now we call `out.backward()`. This triggers the chain rule from the end of the graph back to the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()\n",
    "\n",
    "print(\"Gradients Calculated:\")\n",
    "print(f\"w0.grad: {w0.grad} (Slide Expectation: ~ -0.20)\")\n",
    "print(f\"x0.grad: {x0.grad} (Slide Expectation: ~ 0.40)\")\n",
    "print(f\"w1.grad: {w1.grad} (Slide Expectation: ~ -0.40)\")\n",
    "print(f\"x1.grad: {x1.grad} (Slide Expectation: ~ -0.60)\")\n",
    "print(f\"w2.grad: {w2.grad} (Slide Expectation: ~ 0.20)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis:** compare these numbers to the red numbers in Slide 26. \n",
    "*Note: There might be slight floating point differences or sign differences depending on exactly how the slide defined inputs, but the magnitude should match.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: PyTorch Verification\n",
    "\n",
    "Now we do the exact same thing using PyTorch's `autograd`. This confirms that our tiny engine is mathematically correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 1. Define Inputs (requires_grad=True tells PyTorch to track operations)\n",
    "w0_t = torch.tensor([2.0], requires_grad=True)\n",
    "x0_t = torch.tensor([-1.0], requires_grad=True)\n",
    "w1_t = torch.tensor([-3.0], requires_grad=True)\n",
    "x1_t = torch.tensor([-2.0], requires_grad=True)\n",
    "w2_t = torch.tensor([-3.0], requires_grad=True)\n",
    "\n",
    "# 2. Forward Pass\n",
    "# We use torch.sigmoid to mimic the full operation flow directly\n",
    "z = (w0_t * x0_t) + (w1_t * x1_t) + w2_t\n",
    "out_t = torch.sigmoid(z)\n",
    "\n",
    "print(\"PyTorch Output:\", out_t.item())\n",
    "\n",
    "# 3. Backward Pass\n",
    "out_t.backward()\n",
    "\n",
    "print(\"\\nPyTorch Gradients:\")\n",
    "print(f\"w0.grad: {w0_t.grad.item()}\")\n",
    "print(f\"x0.grad: {x0_t.grad.item()}\")\n",
    "print(f\"w1.grad: {w1_t.grad.item()}\")\n",
    "print(f\"x1.grad: {x1_t.grad.item()}\")\n",
    "print(f\"w2.grad: {w2_t.grad.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "1.  **The Graph:** In PyTorch, you don't see the graph explicitly, but it is built dynamically as you perform operations (like `+` and `*`).\n",
    "2.  **Memory:** Every time you create a node with `requires_grad=True`, PyTorch allocates memory to store the graph. This is why we use `with torch.no_grad():` during validation/testing—to stop building this graph and save memory.\n",
    "3.  **Accumulation:** Gradients accumulate. If we ran `out_t.backward()` again, the gradients would double. This is why we must call `optimizer.zero_grad()` in a training loop."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "342wi26",
   "language": "python",
   "name": "342wi26"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

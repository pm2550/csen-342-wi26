{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 6-3: Identity Crisis â€“ \"Siamese Networks for Re-ID\"\n",
    "\n",
    "**Course:** CSEN 342: Deep Learning  \n",
    "**Topic:** Metric Learning, Siamese Networks, and Contrastive Loss\n",
    "\n",
    "## Objective\n",
    "In standard classification, we teach a network to say \"This is a dog.\" In **Re-Identification (Re-ID)**, we don't know the classes beforehand (e.g., a new person walks into a camera frame). Instead, we must teach the network to answer: \"Are these two images the **same** object?\"\n",
    "\n",
    "This is called **Metric Learning**. We want to learn an embedding space where similar objects are close together and different objects are far apart.\n",
    "\n",
    "In this tutorial, we will:\n",
    "1.  **Build a Siamese Dataset:** Create a data loader that produces *pairs* of images (Positive/Negative examples).\n",
    "2.  **Design a Siamese Network:** A model with two identical branches that share weights.\n",
    "3.  **Implement Contrastive Loss:** A custom loss function that pulls similar pairs together and pushes dissimilar pairs apart.\n",
    "4.  **Visualize Embeddings:** See how the network organizes data in geometric space.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The Siamese Dataset\n",
    "\n",
    "We need a dataset that returns two images and a label ($1$ if same class, $0$ if different). We will use Fashion-MNIST, treating each class (e.g., 'Sneaker') as a distinct 'identity'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import utility functions\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "from utils import download_fashion_mnist\n",
    "\n",
    "download_fashion_mnist()\n",
    "\n",
    "# 1. Define the Paired Dataset\n",
    "class SiameseFashionMNIST(Dataset):\n",
    "    def __init__(self, root, train=True, transform=None):\n",
    "        self.ds = torchvision.datasets.FashionMNIST(root=root, train=train, download=True, transform=transform)\n",
    "        # Group indices by class for fast lookup\n",
    "        self.class_indices = [[] for _ in range(10)]\n",
    "        for idx, (_, label) in enumerate(self.ds):\n",
    "            self.class_indices[label].append(idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Image 1: Get the item at 'index'\n",
    "        img1, label1 = self.ds[index]\n",
    "\n",
    "        # Image 2: 50% chance of same class, 50% chance of diff class\n",
    "        should_get_same_class = random.randint(0, 1)\n",
    "        \n",
    "        if should_get_same_class:\n",
    "            # Pick random index from same class\n",
    "            idx2 = random.choice(self.class_indices[label1])\n",
    "            target = 1.0 # Similar\n",
    "        else:\n",
    "            # Pick random class (that isn't label1)\n",
    "            diff_label = random.randint(0, 9)\n",
    "            while diff_label == label1:\n",
    "                diff_label = random.randint(0, 9)\n",
    "            # Pick random index from that different class\n",
    "            idx2 = random.choice(self.class_indices[diff_label])\n",
    "            target = 0.0 # Dissimilar\n",
    "            \n",
    "        img2, label2 = self.ds[idx2]\n",
    "        return img1, img2, torch.tensor(target, dtype=torch.float32)\n",
    "\n",
    "# Visualize\n",
    "transform = transforms.ToTensor()\n",
    "train_ds = SiameseFashionMNIST(root='../data', train=True, transform=transform)\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "\n",
    "img1, img2, target = next(iter(train_loader))\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "axs[0].imshow(img1[0].squeeze(), cmap='gray')\n",
    "axs[1].imshow(img2[0].squeeze(), cmap='gray')\n",
    "plt.title(f\"Label: {target[0].item()} (1=Same, 0=Diff)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: The Siamese Network\n",
    "\n",
    "A Siamese network passes both images through the **same** embedding network (shared weights). \n",
    "\n",
    "We will force the output to be 2-dimensional (vector of size 2) so we can plot it easily later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmbeddingNet, self).__init__()\n",
    "        # Simple CNN\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 5), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(32, 64, 5), nn.ReLU(), nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64 * 4 * 4, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 2) # Output 2D embedding for easy visualization\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.embedding_net = EmbeddingNet()\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # Pass both images through the SAME network\n",
    "        output1 = self.embedding_net(x1)\n",
    "        output2 = self.embedding_net(x2)\n",
    "        return output1, output2\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SiameseNetwork().to(device)\n",
    "print(\"Siamese Network Initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Contrastive Loss\n",
    "\n",
    "We use the **Contrastive Loss** function.\n",
    "Let $D$ be the Euclidean distance between the two embeddings: $D = || v_1 - v_2 ||_2$.\n",
    "Let $Y$ be the label ($1$ for same, $0$ for different).\n",
    "\n",
    "$$ L = Y \\cdot D^2 + (1 - Y) \\cdot \\max(0, \\text{margin} - D)^2 $$\n",
    "\n",
    "* **If Same ($Y=1$):** We minimize $D^2$ (pull them together).\n",
    "* **If Different ($Y=0$):** We minimize $\\max(0, m - D)^2$. This pushes them apart, but only until they are separated by `margin`. Once they are far enough, we stop caring (loss becomes 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        # Calculate Euclidean Distance\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        \n",
    "        # Contrastive Loss Formula\n",
    "        loss_contrastive = torch.mean((label) * torch.pow(euclidean_distance, 2) + \n",
    "                                      (1 - label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "\n",
    "        return loss_contrastive\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = ContrastiveLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Training and Visualization\n",
    "\n",
    "We will train for a few epochs. After training, we will pass the test set through the network and plot the 2D embeddings. If the network works, images of the same class (e.g., all Shoes) should cluster together in space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "print(\"Starting Training...\")\n",
    "model.train()\n",
    "for epoch in range(5): \n",
    "    total_loss = 0\n",
    "    for i, (img1, img2, label) in enumerate(train_loader):\n",
    "        img1, img2, label = img1.to(device), img2.to(device), label.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        out1, out2 = model(img1, img2)\n",
    "        loss = criterion(out1, out2, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}: Loss {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# Visualization function\n",
    "def plot_embeddings(model):\n",
    "    # Use standard FashionMNIST test set (not pairs)\n",
    "    test_ds = torchvision.datasets.FashionMNIST(root='../data', train=False, transform=transforms.ToTensor())\n",
    "    test_loader = DataLoader(test_ds, batch_size=256, shuffle=False)\n",
    "    \n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for img, label in test_loader:\n",
    "            img = img.to(device)\n",
    "            emb = model.embedding_net(img)\n",
    "            embeddings.append(emb.cpu().numpy())\n",
    "            labels.append(label.numpy())\n",
    "            \n",
    "    embeddings = np.concatenate(embeddings)\n",
    "    labels = np.concatenate(labels)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "    \n",
    "    scatter = plt.scatter(embeddings[:, 0], embeddings[:, 1], c=labels, cmap='tab10', alpha=0.6)\n",
    "    plt.legend(handles=scatter.legend_elements()[0], labels=classes, title=\"Classes\")\n",
    "    plt.title(\"Learned 2D Embeddings via Siamese Network\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "print(\"Visualizing Test Set Embeddings...\")\n",
    "plot_embeddings(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "Look at the plot! \n",
    "* **Clustering:** Notice how similar items (e.g., Sneakers, Sandals, and Boots) group together in one region of the 2D space, while Clothing items group in another.\n",
    "* **Metric Learning:** We never trained the network to classify \"This is a T-Shirt.\" We only taught it \"These two look alike.\" Yet, it naturally discovered the class structure.\n",
    "    \n",
    "This is the foundation of modern Face ID, Re-ID, and Image Retrieval systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 8-1: Beyond Random Initialization â€“ Transfer Learning with Word Embeddings\n",
    "\n",
    "**Course:** CSEN 342: Deep Learning  \n",
    "**Topic:** Word Embeddings, `nn.Embedding`, Transfer Learning, and Text Classification\n",
    "\n",
    "## Objective\n",
    "In the lecture, we learned that Word Embeddings (like Word2Vec or GloVe) capture semantic meaning. We also discussed **Transfer Learning** (Slide 17): the idea of taking these embeddings trained on billions of words and applying them to a smaller task.\n",
    "\n",
    "In this tutorial, we will verify this experimentally using a **Real Dataset** (Subjectivity vs. Objectivity Analysis). We will train three models:\n",
    "1.  **From Scratch:** Initialize embeddings randomly. The model knows nothing about English initially.\n",
    "2.  **Static Transfer (Frozen):** Load GloVe embeddings and **freeze** them. The model only learns the classifier layer.\n",
    "3.  **Fine-Tuning:** Initialize with GloVe but allow the embeddings to update during training.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Setup (Subjectivity Dataset)\n",
    "\n",
    "We will use the **Subjectivity Dataset v1.0** (Pang/Lee 2004). It contains 5,000 subjective sentences (opinions) and 5,000 objective sentences (facts).\n",
    "\n",
    "**Task:** Classify a sentence as Subjective (1) or Objective (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import os\n",
    "import re\n",
    "import tarfile\n",
    "\n",
    "# 1. Download Data (Subjectivity Dataset)\n",
    "data_root = '../data'\n",
    "os.makedirs(data_root, exist_ok=True)\n",
    "subj_url = \"http://www.cs.cornell.edu/people/pabo/movie-review-data/rotten_imdb.tar.gz\"\n",
    "tar_path = os.path.join(data_root, 'rotten_imdb.tar.gz')\n",
    "extract_path = os.path.join(data_root, 'rotten_imdb')\n",
    "\n",
    "if not os.path.exists(extract_path):\n",
    "    print(\"Downloading Subjectivity Dataset...\")\n",
    "    # Use wget with -nc (no clobber) to avoid re-downloading if exists\n",
    "    os.system(f\"wget -nc -P {data_root} {subj_url}\")\n",
    "    \n",
    "    print(\"Extracting...\")\n",
    "    with tarfile.open(tar_path, \"r:gz\") as tar:\n",
    "        tar.extractall(path=data_root)\n",
    "    # The tar extracts to a folder named 'rotten_imdb' usually containing 'quote.tok.gt9.5000' and 'plot.tok.gt9.5000'\n",
    "    # We rename it to be clean if needed, or just set paths directly\n",
    "    print(\"Done.\")\n",
    "\n",
    "# 2. Load and Label Data\n",
    "subj_file = os.path.join(data_root, 'quote.tok.gt9.5000') # Subjective (1)\n",
    "obj_file = os.path.join(data_root, 'plot.tok.gt9.5000')  # Objective (0)\n",
    "\n",
    "# Read files (handle latin-1 encoding common in older datasets)\n",
    "with open(subj_file, 'r', encoding='latin-1') as f:\n",
    "    subj_lines = [(line.strip(), 1) for line in f]\n",
    "    \n",
    "with open(obj_file, 'r', encoding='latin-1') as f:\n",
    "    obj_lines = [(line.strip(), 0) for line in f]\n",
    "\n",
    "# Combine and Shuffle\n",
    "all_data = subj_lines + obj_lines\n",
    "np.random.shuffle(all_data)\n",
    "\n",
    "print(f\"Total samples: {len(all_data)}\")\n",
    "print(\"Sample 1:\", all_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Building the Vocabulary\n",
    "We convert text to integers. We reserve `0` for padding and `1` for unknown words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    return re.findall(r\"\\w+\", text.lower())\n",
    "\n",
    "def build_vocab(data, max_size=10000):\n",
    "    counter = Counter()\n",
    "    for text, _ in data:\n",
    "        counter.update(tokenizer(text))\n",
    "            \n",
    "    vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "    # Add most common words\n",
    "    for word, _ in counter.most_common(max_size - 2):\n",
    "        vocab[word] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "vocab = build_vocab(all_data)\n",
    "print(f\"Vocabulary size: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Loading Pre-trained Embeddings (GloVe)\n",
    "\n",
    "We create an embedding matrix where row `i` corresponds to the GloVe vector for the word at index `i`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download GloVe if needed\n",
    "glove_path = os.path.join(data_root, 'glove.6B.50d.txt')\n",
    "if not os.path.exists(glove_path):\n",
    "    print(\"Downloading GloVe...\")\n",
    "    os.system(f\"wget -nc -P {data_root} https://huggingface.co/stanfordnlp/glove/resolve/main/glove.6B.zip\")\n",
    "    os.system(f\"unzip -o {data_root}/glove.6B.zip -d {data_root}\")\n",
    "\n",
    "def create_embedding_matrix(vocab, glove_path, emb_dim=50):\n",
    "    # 1. Load GloVe into a dictionary\n",
    "    glove_dict = {}\n",
    "    with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            glove_dict[word] = vector\n",
    "            \n",
    "    # 2. Create matrix initialized randomly\n",
    "    matrix = np.random.normal(scale=0.6, size=(len(vocab), emb_dim))\n",
    "    matrix[0] = np.zeros(emb_dim) # <PAD> is zero\n",
    "    \n",
    "    hits = 0\n",
    "    for word, idx in vocab.items():\n",
    "        if word in glove_dict:\n",
    "            matrix[idx] = glove_dict[word]\n",
    "            hits += 1\n",
    "            \n",
    "    print(f\"GloVe coverage: {hits}/{len(vocab)} words found ({hits/len(vocab):.1%})\")\n",
    "    return torch.tensor(matrix, dtype=torch.float32)\n",
    "\n",
    "embedding_weights = create_embedding_matrix(vocab, glove_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: The Model (Deep Averaging Network)\n",
    "\n",
    "We use a simple architecture that averages word vectors and passes them through a classifier. This is often called a **Deep Averaging Network (DAN)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, pretrained_weights=None, freeze=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initialize Embedding Layer\n",
    "        if pretrained_weights is not None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(pretrained_weights, padding_idx=0, freeze=freeze)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        \n",
    "        # Classifier Head\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(emb_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (Batch, Seq_Len)\n",
    "        embeds = self.embedding(x)\n",
    "        # Average Pooling (mean of all words in sentence)\n",
    "        pooled = embeds.mean(dim=1)\n",
    "        return self.fc(pooled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: The Experiment\n",
    "\n",
    "We train three models to compare the impact of initialization strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubjectivityDataset(Dataset):\n",
    "    def __init__(self, data, vocab, max_len=30):\n",
    "        self.processed_data = []\n",
    "        for text, label in data:\n",
    "            tokens = tokenizer(text)\n",
    "            # Map to integers\n",
    "            indices = [vocab.get(t, vocab[\"<UNK>\"]) for t in tokens]\n",
    "            # Pad or Truncate\n",
    "            if len(indices) < max_len:\n",
    "                indices += [vocab[\"<PAD>\"]] * (max_len - len(indices))\n",
    "            else:\n",
    "                indices = indices[:max_len]\n",
    "            self.processed_data.append((torch.tensor(indices), int(label)))\n",
    "\n",
    "    def __len__(self): return len(self.processed_data)\n",
    "    def __getitem__(self, idx): return self.processed_data[idx]\n",
    "\n",
    "# Create Dataset and Loader\n",
    "# Split 80/20\n",
    "split_idx = int(len(all_data) * 0.8)\n",
    "train_data = all_data[:split_idx]\n",
    "val_data = all_data[split_idx:]\n",
    "\n",
    "train_ds = SubjectivityDataset(train_data, vocab)\n",
    "val_ds = SubjectivityDataset(val_data, vocab)\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=100)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def train_evaluate(strategy_name, weights=None, freeze=False):\n",
    "    model = TextClassifier(len(vocab), 50, weights, freeze).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    val_accuracies = []\n",
    "    \n",
    "    for epoch in range(15):\n",
    "        model.train()\n",
    "        for text, labels in train_loader:\n",
    "            text, labels = text.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(text)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        # Evaluate\n",
    "        model.eval()\n",
    "        correct = 0; total = 0\n",
    "        with torch.no_grad():\n",
    "            for text, labels in val_loader:\n",
    "                text, labels = text.to(device), labels.to(device)\n",
    "                output = model(text)\n",
    "                _, predicted = torch.max(output, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        acc = 100 * correct / total\n",
    "        val_accuracies.append(acc)\n",
    "        \n",
    "    return val_accuracies\n",
    "\n",
    "print(\"Training 1: Random Init (From Scratch)...\")\n",
    "acc_scratch = train_evaluate(\"Scratch\", weights=None)\n",
    "\n",
    "print(\"Training 2: Static GloVe (Frozen)...\")\n",
    "acc_frozen = train_evaluate(\"Frozen\", weights=embedding_weights, freeze=True)\n",
    "\n",
    "print(\"Training 3: Fine-Tuning GloVe...\")\n",
    "acc_finetune = train_evaluate(\"Fine-Tune\", weights=embedding_weights, freeze=False)\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(acc_scratch, label='Random Init', linestyle='--')\n",
    "plt.plot(acc_frozen, label='GloVe (Frozen)')\n",
    "plt.plot(acc_finetune, label='GloVe (Fine-Tuned)', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Accuracy (%)')\n",
    "plt.title('Transfer Learning on Subjectivity Dataset')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "Because we used a real dataset (Subjectivity 1.0) with moderate size (10k samples), you should see clear trends:\n",
    "\n",
    "1.  **Frozen GloVe:** Starts with high accuracy (jump start) because the embeddings already understand language, but it might plateau lower if the task-specific words (like \"touching\" or \"masterpiece\" in movie reviews) need to shift.\n",
    "2.  **Fine-Tuned GloVe:** Typically wins. It starts high (transfer learning) and continues to improve as it adapts the generic GloVe vectors to the specific nuances of movie review sentiment.\n",
    "3.  **Random Init:** Starts near 50% (guessing) and has to learn English from scratch. It will eventually catch up, but takes longer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
